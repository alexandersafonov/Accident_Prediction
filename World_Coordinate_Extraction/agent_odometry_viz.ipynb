{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"agent_odometry_viz.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"RMfBJ5QtylaU","colab_type":"code","outputId":"531b7380-8320-41cf-dfba-01bc22e56fb9","executionInfo":{"status":"ok","timestamp":1581747054708,"user_tz":480,"elapsed":101899,"user":{"displayName":"ALEXANDER SAFONOV","photoUrl":"","userId":"06108573154481941124"}},"colab":{"base_uri":"https://localhost:8080/","height":805}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","%cd /content/gdrive/My\\ Drive/Accident_Anticipation/depth_estimation/google_distance_code/google-research/depth_from_video_in_the_wild\n","from image_utils import *\n","\n","%cd /content/gdrive/My\\ Drive/Accident_Anticipation/depth_estimation/google_distance_code/google-research\n","\n","!pip install tensorflow-graphics\n","\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","\n","from __future__ import absolute_import, division, print_function\n","from absl import logging\n","import os\n","# import image_utils\n","import cv2\n","import re\n","import time\n","import glob\n","from matplotlib import pyplot as plt\n","import pickle\n","import numpy as np\n","from PIL import Image\n","import pandas as pd\n","from google.colab.patches import cv2_imshow\n","\n","import tensorflow as tf\n","from depth_from_video_in_the_wild import model\n","\n","# https://github.com/google-research/google-research/tree/master/depth_from_video_in_the_wild"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n","/content/gdrive/My Drive/Accident_Anticipation/depth_estimation/google_distance_code/google-research/depth_from_video_in_the_wild\n","/content/gdrive/My Drive/Accident_Anticipation/depth_estimation/google_distance_code/google-research\n","Collecting tensorflow-graphics\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/5d/b84b85322723aef25e5069596438a92dc7d5d1035ee11947e2105315eaa2/tensorflow_graphics-1.0.0-py2.py3-none-any.whl (243kB)\n","\u001b[K     |████████████████████████████████| 245kB 2.9MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-graphics) (1.12.0)\n","Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-graphics) (0.9.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-graphics) (1.4.1)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow-graphics) (1.17.5)\n","Requirement already satisfied: tensorflow>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-graphics) (1.15.0)\n","Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1->tensorflow-graphics) (1.15.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1->tensorflow-graphics) (0.34.2)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1->tensorflow-graphics) (1.1.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1->tensorflow-graphics) (0.8.1)\n","Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1->tensorflow-graphics) (0.2.2)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1->tensorflow-graphics) (3.10.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1->tensorflow-graphics) (3.1.0)\n","Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1->tensorflow-graphics) (1.15.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1->tensorflow-graphics) (1.1.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1->tensorflow-graphics) (1.27.1)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1->tensorflow-graphics) (1.11.2)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1->tensorflow-graphics) (0.1.8)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1->tensorflow-graphics) (1.0.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow>=1.13.1->tensorflow-graphics) (45.1.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.13.1->tensorflow-graphics) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.13.1->tensorflow-graphics) (3.2.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow>=1.13.1->tensorflow-graphics) (2.8.0)\n","Installing collected packages: tensorflow-graphics\n","Successfully installed tensorflow-graphics-1.0.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Warning: To use the exr data format, please install the OpenEXR package following the instructions detailed in the README at github.com/tensorflow/graphics.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gmO8yvnEBPWg","colab_type":"text"},"source":["Feature and Image Paths:"]},{"cell_type":"code","metadata":{"id":"-CU3C101ANlD","colab_type":"code","colab":{}},"source":["cropped_img_path = '/content/gdrive/My Drive/Accident_Anticipation/depth_estimation/google_distance_code/accident_data/reformatting_imgs/reordered_final_outputs_2/'\n","optimal_crop_path = '/content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/preprocessed_features/optimal_crops_det_2/' \n","# path to depth_from_the_wild outputs\n","path_odom = '/content/gdrive/My Drive/Accident_Anticipation/depth_estimation/google_distance_code/google-research/depth_from_video_in_the_wild/odometry_trajectory_outputs/'\n","path_depth = '/content/gdrive/My Drive/Accident_Anticipation/depth_estimation/depth_maps/distance_maps/'\n","\n","root = '/content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents'\n","# path\n","train_path = root + '/dataset/features/training/' \n","test_path = root +  '/dataset/features/testing/' \n","\n","###\n","batch_str = 'batch_045.npz'\n","mode = 'training'\n","vid_num = 2\n","###\n","\n","num1 = (126, 46)\n","org_bb = np.load(train_path+batch_str) # use org_bb to retrieve bounding boxes, and the correct video ID\n","resized_bb = np.load(optimal_crop_path+'training/'+batch_str)['new_det']\n","# track_ids = np.load(path_track_ids+'training/'+batch_str)\n","frame_paths, frame_labels = retrieve_dir1(org_bb['labels'], org_bb['ID'],mode, num1[0])\n","odom_complete_paths = retrieve_dir_custom(path_odom, org_bb, 'training', num1[0], split_type=1)\n","depth_complete_paths = retrieve_dir_custom(path_depth, org_bb, 'training', num1[0])\n","\n","depth_np = np.load(depth_complete_paths[vid_num]) # ['distance_map']\n","odom_np = np.load(odom_complete_paths[vid_num]) # ['rot', 'trans', 'instrinsic_matrix', 'orientation', 'position']\n","\n","cur_vid_folder = re.split('/', frame_paths[vid_num])[-3]+'_'+re.split('/', frame_paths[vid_num])[-2]\n","cur_img_path = cropped_img_path+'/'+'training'+'/' + cur_vid_folder\n","# create list\n","folders = sorted(os.listdir(cur_img_path))\n","img_list = [cur_img_path +'/' + f for f in folders if 'png' in f and 'seg' not in f]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8MJlzLPrqGoU","colab_type":"code","colab":{}},"source":["new_save_path = '/content/gdrive/My Drive/Accident_Anticipation/EXTRACT_PHYSICS_NOTEBOOKS/physical_features/5_world_coordinates_4v/'\n","\n","num1 = (126, 46)\n","\n","def retrieve_global_coord(global_df_path, feat_path, batch_str, num_samp, mode='training'):\n","  glob_feat = np.zeros((10,100, 20, 4))\n","  org_bb = np.load(feat_path+batch_str) # use org_bb to retrieve bounding boxes, and the correct video ID\n","  frame_paths, frame_labels = retrieve_dir1(org_bb['labels'], org_bb['ID'], mode, num_samp)\n","  global_df_paths = retrieve_dir_custom(new_save_path, org_bb, mode, num_samp, split_type=2)\n"," \n","  for batch_num in range(10):\n","    cur_df_path = global_df_paths[batch_num]\n","    # add try except functionallity in case df doesn't exisit\n","    try:\n","      trajectory_df = pd.read_pickle(cur_df_path)\n","      time_stamps = np.linspace(0,5,100) # use time_stamps to retrieve the correct index\n","      frames = trajectory_df.groupby('TIMESTAMPS')\n","\n","      for group_name, group_data in frames:\n","        time_ind = np.argwhere(time_stamps == group_name)[0][0] \n","        cur_x, cur_y = group_data['X'], group_data['Z']\n","        v_x, v_y = group_data['v_x'], group_data['v_y']\n","        glob_feat[batch_num, time_ind,0:len(cur_x),0] = cur_x\n","        glob_feat[batch_num, time_ind,0:len(cur_x),1] = cur_y\n","        glob_feat[batch_num, time_ind,0:len(cur_x),2] = v_x\n","        glob_feat[batch_num, time_ind,0:len(cur_x),3] = v_y\n","        # 4 dimensions\n","        # 4 velocity dimensions\n","    except:\n","      print(\"Dataframe doesn't exist: \" + str(cur_df_path))\n","\n","\n","  # world_coordinate modification\n","  # glob_feat = take_out_nans(glob_feat)\n","  # glob_feat[glob_feat == np.inf] = 0\n","  # glob_feat[glob_feat*(-1) == np.inf] = 0\n","  # glob_feat = glob_feat*0.1  ## change/normalize the global features\n","\n","  return glob_feat \n","\n","def take_out_nans(phys_feat):\n","    if np.sum(np.isnan(phys_feat)) > 0:\n","      nan_indices = np.argwhere(np.isnan(phys_feat))\n","      for k in range(nan_indices.shape[0]):\n","        phys_feat[nan_indices[k,0],nan_indices[k,1],nan_indices[k,2],nan_indices[k,3]] = 0\n","    return phys_feat\n","\n","def show_agents_visualization(resized_bb, cropped_img_path, vid_num, batch_str, frame_paths, trajectory_df, k_width = 12, mode= 'training'):\n","  cur_vid_folder = re.split('/', frame_paths[vid_num])[-3]+'_'+re.split('/', frame_paths[vid_num])[-2]\n","  cur_img_path = cropped_img_path+'/'+mode+'/' + cur_vid_folder\n","  folders = sorted(os.listdir(cur_img_path))\n","  img_list = [cur_img_path +'/' + f for f in folders if 'png' in f and 'seg' not in f]\n","  bboxes = resized_bb[vid_num]\n","  crop_type = np.max(np.max(bboxes[:,:,-2]))\n","  str_path = cropped_img_path+mode\n","  folders = sorted(os.listdir(str_path))\n","  # rgb_path = '../../Anticipating-Accidents/dataset/videos/frames/'+a1 +'/'+a2 +'/'+cur_file + '/'\n","  # video_path =  '../../Anticipating-Accidents/dataset/videos/'+a1 +'/'+a2 +'/'+cur_file +'.mp4'\n","  fig, ax = plt.subplots()\n","  max_agent_ind = np.max(bboxes[:,:,-1])\n","  frames = trajectory_df.groupby('Agent_ID')\n","\n","  for img_ind in range(98):\n","      cur_img_pth = img_list[img_ind]\n","      # crop image to dimensions\n","      # cur_img_pth = rgb_path +'frame'+str(img_ind)+'.jpg' ############ change this name\n","      frame = cv2.imread(cur_img_pth)\n","      frame = frame[:, 0:416, :] # crop images, since three are fused together for training purposes\n","      new_bboxes = bboxes[img_ind]\n","      new_bboxes = new_bboxes[~np.all(new_bboxes == 0, axis=-1)] ## take out zero values from new_bboxes  \n","      #############################\n","      ## Replace with plotting function\n","      agents_ids = new_bboxes[:,-1]\n","\n","      ax = _add_traj(frames, img_ind, ax, k_width = k_width)\n","      fig.canvas.draw()\n","      img = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8,sep='')\n","      img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n","      img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n","\n","      font = cv2.FONT_HERSHEY_SIMPLEX # change font type?\n","  #     frame = cv2.imread(cur_img_pth)\n","      for num_box in range(new_bboxes.shape[0]):\n","          cv2.rectangle(frame,(int(new_bboxes[num_box,0]),int(new_bboxes[num_box,1])),(int(new_bboxes[num_box,2]),int(new_bboxes[num_box,3])),(0,255,0),1)\n","          # add agent ids in top right-hand corner\n","          cv2.putText(frame,str(new_bboxes[num_box,-1]),(int(new_bboxes[num_box,0]),int(new_bboxes[num_box,1])), font, 0.3,(0,0,255),1,cv2.LINE_AA) ## put physical parameters here\n","\n","      top_left_label = str(img_ind) + ': ' + str(crop_type) +', ' + cur_vid_folder\n","      cv2.putText(frame,top_left_label ,(10,30), font, 0.25,(255,255,255),1)\n","      # cv2_imshow(frame)\n","      plt_img = cv2.resize(img,(400,200))\n","      plt_img = cv2.hconcat([plt_img, plt_img])\n","      frame_resized = cv2.resize(frame, (800, 200))\n","      cv2_imshow(cv2.vconcat([plt_img, frame_resized]))\n","  #         plt.show()\n","\n","\n","def _add_traj(frames, k, ax, k_width = 12):\n","    time_stamps = np.linspace(0,5,100)\n","    min_x, max_x = np.min(np.min(frames['X'])), np.max(np.max(frames['X']))\n","    min_z, max_z = np.min(np.min(frames['Z'])), np.max(np.max(frames['Z']))\n","    ax.clear()\n","    ax.set_xlim(min_x, max_x)\n","    ax.set_ylim(min_z, max_z)\n","    ax.set_title('Frame '+ str(k))\n","    for group_name, group_data in frames:\n","      past_k = k - k_width\n","      if past_k < 0:\n","        past_k = 0\n","      mask = (group_data['TIMESTAMPS'].values> time_stamps[past_k])*(group_data['TIMESTAMPS'].values < time_stamps[k])\n","      true_indices = [i for i, x in enumerate(mask) if x]\n","      if len(true_indices)>0:\n","        new_mask = mask\n","        new_mask[true_indices] = False\n","        new_mask[true_indices[-1]] = True\n","      else:\n","        new_mask = mask\n","      if group_name != 0:\n","        cor_x = group_data[\"X\"][mask].values\n","        cor_y = group_data[\"Z\"][mask].values\n","        ax.plot(\n","        cor_x,\n","        cor_y,\n","        \".\")\n","        # if len(true_indices)>0:\n","        #   p_x = np.asscalar(group_data[\"X\"][new_mask].values)\n","        #   p_y = np.asscalar(group_data[\"Z\"][new_mask].values)\n","        #   v_x = np.asscalar(group_data[\"v_x\"][new_mask].values)\n","        #   v_y = np.asscalar(group_data[\"v_y\"][new_mask].values)\n","        #   print('Inputs:')\n","        #   print(p_x, p_y,v_x, v_y)\n","        #   ax.arrow(\n","        #   p_x,\n","        #   p_y,\n","        #   v_x,\n","        #   v_y)\n","      else:\n","        ego_x = group_data[\"X\"][0:k].values\n","        ego_y = group_data[\"Z\"][0:k].values\n","        ax.plot(\n","        ego_x,\n","        ego_y,\n","        \"-\") \n","    return ax\n","\n","def full_visualization(det, vid_num, batch_str, frame_paths, mode= 'training'):\n","  resized_bb = det\n","  cur_img_path = frame_paths[vid_num]\n","  print(cur_img_path)\n","  img_list = sort_dir(os.listdir(cur_img_path))\n","  print(img_list)\n","  bboxes = resized_bb[vid_num]\n","  crop_type = np.max(np.max(bboxes[:,:,-2]))\n","\n","  for img_ind in range(98):\n","      cur_img_pth = cur_img_path+img_list[img_ind]\n","      print(cur_img_path)\n","      frame = cv2.imread(cur_img_pth)\n","      new_bboxes = bboxes[img_ind]\n","      new_bboxes = new_bboxes[~np.all(new_bboxes == 0, axis=1)] ## take out zero values from new_bboxes  \n","      agents_ids = new_bboxes[:,-1]\n","      font = cv2.FONT_HERSHEY_SIMPLEX # change font type?\n","\n","      for num_box in range(new_bboxes.shape[0]):\n","          # print(img_ind)\n","          # print(new_bboxes[num_box,0],new_bboxes[num_box,1],new_bboxes[num_box,2],new_bboxes[num_box,3] )\n","          cv2.rectangle(frame,(int(new_bboxes[num_box,0]),int(new_bboxes[num_box,1])),(int(new_bboxes[num_box,2]),int(new_bboxes[num_box,3])),(0,255,0),2)\n","          # add agent ids in top right-hand corner\n","          cv2.putText(frame,str(new_bboxes[num_box,-1]),(int(new_bboxes[num_box,0]),int(new_bboxes[num_box,1])), font, 0.5,(0,0,255),2,cv2.LINE_AA) ## put physical parameters here\n","\n","      top_left_label = str(img_ind) + ': ' + str(crop_type) +', ' + cur_vid_folder\n","      cv2.putText(frame,top_left_label ,(10,30), font, 0.5,(255,255,255),2)\n","      frame_resized = cv2.resize(frame, (800, 400))\n","      cv2_imshow(frame_resized)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"e88370c5-4182-4e7a-aa17-814b296ac93f","executionInfo":{"status":"ok","timestamp":1581747229299,"user_tz":480,"elapsed":48747,"user":{"displayName":"ALEXANDER SAFONOV","photoUrl":"","userId":"06108573154481941124"}},"id":"W9NV52X9uQ5W","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1hCoFkMiOPJnBCvY4ggQ7ioFWmANnDcxG"}},"source":["batch_str = 'batch_060.npz'\n","mode = 'training'\n","vid_num =5\n","\n","num1 = (126, 46)\n","org_bb = np.load(train_path+batch_str) # use org_bb to retrieve bounding boxes, and the correct video ID\n","frame_paths, frame_labels = retrieve_dir1(org_bb['labels'], org_bb['ID'],mode, num1[0])\n","pkl_str = re.split('/',frame_paths[vid_num])[-3]+'_'+re.split('/',frame_paths[vid_num])[-2]+'.pkl'\n","# save_path = '/content/gdrive/My Drive/Accident_Anticipation/EXTRACT_PHYSICS_NOTEBOOKS/physical_features/5_world_coordinates_2/'\n","save_path = '/content/gdrive/My Drive/Accident_Anticipation/EXTRACT_PHYSICS_NOTEBOOKS/physical_features/5_world_coordinates_3v/'\n","cropped_img_path = '/content/gdrive/My Drive/Accident_Anticipation/depth_estimation/google_distance_code/accident_data/reformatting_imgs/reordered_final_outputs_2/'\n","optimal_crop_path = '/content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/preprocessed_features/optimal_crops_det_2/' \n","\n","training_path = '/content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/training-sort/output_sort9AGE=2IOU=0.3/'\n","testing_path = '/content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing-sort/output_sort10AGE=2IOU=0.3/'\n","\n","\n","trn_path = '/content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/training-sort/output_Sort2bAGE=3IOU=0.15NPZSort/'\n","\n","trn_batch = np.load(training_path+'/'+batch_str)\n","###! DATA IS VISUALIZED HERE!!!!\n","mod_bb = np.load(optimal_crop_path+'/'+mode+'/'+batch_str)\n","\n","# file = open(save_path+pkl_str, 'rb')\n","# trajectory_df = pickle.load(file)\n","# file.close()\n","\n","trajectory_df = pd.read_pickle(save_path+pkl_str)\n","\n","show_agents_visualization(mod_bb['new_det'], cropped_img_path, vid_num, batch_str, frame_paths, trajectory_df, k_width = 20, mode= 'training')"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}