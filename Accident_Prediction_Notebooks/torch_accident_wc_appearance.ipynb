{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UrKWmxee-nt0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on colab\n",
      "/home/sasha/Public/Lab_Stuff/2016_ACCV_Acc_Anticipation/AnticipatingAccidentsCleaned\n",
      "['model', 'Fixed_Track_IDs', 'Accident_Prediction_Notebooks', 'Features', 'World_Coordinate_Extraction']\n",
      "/home/sasha/Public/Lab_Stuff/2016_ACCV_Acc_Anticipation/AnticipatingAccidentsCleaned\r\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "except:\n",
    "    print('Not running on colab')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "%cd /home/sasha/Public/Lab_Stuff/2016_ACCV_Acc_Anticipation/AnticipatingAccidentsCleaned\n",
    "cur_dir = %pwd\n",
    "sys.path.append(cur_dir)\n",
    "\n",
    "nb_path = \"/home/sasha/Public/Lab_Stuff/2016_ACCV_Acc_Anticipation/AnticipatingAccidentsCleaned/Accident_Prediction_Notebooks/utils/WC_appearance_4.py\"\n",
    "# !pip install --target=$nb_path cupy pynvrtc\n",
    "sys.path.insert(0, nb_path)\n",
    "\n",
    "## update root path if downloaded\n",
    "root_path = '/home/sasha/Public/Lab_Stuff/2016_ACCV_Acc_Anticipation/AnticipatingAccidentsCleaned'\n",
    "\n",
    "print(os.listdir('./'))\n",
    "!pwd\n",
    "\n",
    "\n",
    "# warnings.filterwarnings(action='once')\n",
    "warnings.filterwarnings('ignore')\n",
    "# ! pip install cupy pynvrtc\n",
    "\n",
    "\n",
    "#### Attention Learning Blogs\n",
    "# Pytorch attention implementations\n",
    "# https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1295,
     "status": "error",
     "timestamp": 1583304342571,
     "user": {
      "displayName": "ALEXANDER SAFONOV",
      "photoUrl": "",
      "userId": "06108573154481941124"
     },
     "user_tz": 480
    },
    "id": "s7n0HcaPT_YE",
    "outputId": "72ad0b94-6703-454c-a4b8-4d8acfd29675"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pdb\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from itertools import permutations\n",
    "from accident_physical_fgsm_lighter import evaluation #_2 as evaluation\n",
    "# from qrnn import QRNN\n",
    "import pickle \n",
    "import pandas as pd\n",
    "from torchqrnn import QRNN\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# path\n",
    "train_path = root_path + '/Features/original_features/training/' #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Change this line\n",
    "test_path = root_path +'/Features/original_features/testing/'   #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Change this line\n",
    "demo_path = root_path +'/Features/original_features/testing/'   #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Change this line\n",
    "\n",
    "new_save_path = root_path + '/Features/physical_features/5_world_coordinates_5npz'\n",
    "\n",
    "default_model_path = './model/WC_appearance_4_1/'   #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Change this line\n",
    "save_path = './model/WC_appearance_4_1/'  #+ dir_str +'/'                   #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Change this line\n",
    "video_path = './dataset/videos/testing/positive/'\n",
    "\n",
    "# batch_number\n",
    "train_num = 126\n",
    "test_num = 46\n",
    "\n",
    "\n",
    "############## Train Parameters #################\n",
    "# Parameters\n",
    "learning_rate = 0.0001\n",
    "n_epochs = 110\n",
    "batch_size = 10\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 4096 # fc6 or fc7(1*4096)          #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Change this line\n",
    "n_detection = 20 # number of object of each image (include image features)\n",
    "\n",
    "n_phys_hidden = 50\n",
    "phys_dim = 50\n",
    "n_img_hidden = 200 #20 + phys_dim # embedding image features \n",
    "n_att_hidden = 200 #20 + phys_dim # embedding object features\n",
    "n_classes = 2 # has accident or not\n",
    "n_frames = 100 # number of frame in each video \n",
    "n_hidden = n_img_hidden+n_att_hidden+n_phys_hidden*2 #300 #+25*2\n",
    "\n",
    "p_drop = 0.5\n",
    "\n",
    "num1 = (126, 46)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wHFMG0FqUXRa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "## change function that we draw from\n",
    "from WC_appearance_4 import EmbedLayer, Attention_LSTM_2, Linear_Pred\n",
    "from WC_appearance_4 import soft_max_entropy_loss_logits, temp_loss, run_batch, approx_evaluation, test_all\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  model1 = EmbedLayer(n_input, n_img_hidden, n_att_hidden, n_hidden, n_phys_hidden, p_drop).cuda()\n",
    "  lstm = Attention_LSTM_2(batch_size,n_hidden,n_hidden, p_drop).cuda()\n",
    "  linear_pred = Linear_Pred(batch_size,n_hidden,n_classes).cuda()\n",
    "else:\n",
    "  model1 = EmbedLayer(n_input, n_img_hidden, n_att_hidden, n_hidden, n_phys_hidden, p_drop)\n",
    "  lstm = Attention_LSTM_2(batch_size,n_hidden,n_hidden, p_drop)\n",
    "  linear_pred = Linear_Pred(batch_size,n_hidden,n_classes)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(model1.parameters())+list(lstm.parameters())+list(linear_pred.parameters()), lr = 0.00005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2254,
     "status": "ok",
     "timestamp": 1582174325603,
     "user": {
      "displayName": "ALEXANDER SAFONOV",
      "photoUrl": "",
      "userId": "06108573154481941124"
     },
     "user_tz": 480
    },
    "id": "ooiDXyASyzsR",
    "outputId": "64bf3605-296e-487d-ab01-b7cb670e1f3f"
   },
   "outputs": [],
   "source": [
    "len_dir = len(os.listdir(save_path))/3\n",
    "ind = (len_dir-1)*3\n",
    "tup = (ind, ind+1, ind+2)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    map_location=lambda storage, loc: storage.cuda()\n",
    "else:\n",
    "    map_location='cpu'\n",
    "\n",
    "map_location=torch.device('cpu')\n",
    "model1.load_state_dict(torch.load(save_path + 'model1_85.pth', map_location = map_location))\n",
    "lstm.load_state_dict(torch.load(save_path + 'model2_85.pth', map_location = map_location))\n",
    "linear_pred.load_state_dict(torch.load(save_path + 'model3_85.pth', map_location = map_location  ))\n",
    "\n",
    "saved_epoch = 86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSJtwIqtUmga"
   },
   "source": [
    "Training Loop, Saving Every 15 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7844687,
     "status": "error",
     "timestamp": 1582183990797,
     "user": {
      "displayName": "ALEXANDER SAFONOV",
      "photoUrl": "",
      "userId": "06108573154481941124"
     },
     "user_tz": 480
    },
    "id": "Qso7r3nsUh6c",
    "outputId": "70d9b956-aa83-4d1f-f3b4-8ac3bed861df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/training/\n",
      "Average Precision= 0.7162 ,mean Time to accident= 3.108\n",
      "Recall@80%, Time to accident= 3.356\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.4128 ,mean Time to accident= 3.521\n",
      "Recall@80%, Time to accident= 3.697\n",
      "Epoch: 87  done. Loss: 0.5294804654187626\n",
      "Epoch Time Cost: 1191.74 s\n",
      " :::::: \n",
      "Testing\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.5435, Mean time to accident= 3.233, Adjusted AATC = 2.403\n",
      "Recall@80%, Time to accident = 3.699\n",
      "Recall@80%, Precision = 0.4045\n",
      " :::::: \n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/training/\n",
      "Average Precision= 0.7314 ,mean Time to accident= 3.143\n",
      "Recall@80%, Time to accident= 3.397\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.4169 ,mean Time to accident= 3.695\n",
      "Recall@80%, Time to accident= 4.056\n",
      "Epoch: 88  done. Loss: 0.5304419079470256\n",
      "Epoch Time Cost: 1242.82 s\n",
      " :::::: \n",
      "Testing\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.5497, Mean time to accident= 3.432, Adjusted AATC = 2.561\n",
      "Recall@80%, Time to accident = 4.065\n",
      "Recall@80%, Precision = 0.3969\n",
      " :::::: \n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/training/\n",
      "Average Precision= 0.7403 ,mean Time to accident= 3.271\n",
      "Recall@80%, Time to accident= 3.528\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.4117 ,mean Time to accident= 3.694\n",
      "Recall@80%, Time to accident= 3.793\n",
      "Epoch: 89  done. Loss: 0.9130627098537626\n",
      "Epoch Time Cost: 1240.82 s\n",
      " :::::: \n",
      "Testing\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.5389, Mean time to accident= 3.325, Adjusted AATC = 2.461\n",
      "Recall@80%, Time to accident = 3.794\n",
      "Recall@80%, Precision = 0.4058\n",
      " :::::: \n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/training/\n",
      "Average Precision= 0.6992 ,mean Time to accident= 3.385\n",
      "Recall@80%, Time to accident= 3.74\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.4052 ,mean Time to accident= 3.715\n",
      "Recall@80%, Time to accident= 3.968\n",
      "Epoch: 90  done. Loss: 0.5823283107980847\n",
      "Epoch Time Cost: 1216.34 s\n",
      " :::::: \n",
      "Testing\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.5344, Mean time to accident= 3.448, Adjusted AATC = 2.547\n",
      "Recall@80%, Time to accident = 3.969\n",
      "Recall@80%, Precision = 0.4058\n",
      " :::::: \n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/training/\n",
      "Average Precision= 0.6700 ,mean Time to accident= 3.303\n",
      "Recall@80%, Time to accident= 1.94\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.3901 ,mean Time to accident= 3.784\n",
      "Recall@80%, Time to accident= 3.991\n",
      "Epoch: 91  done. Loss: 0.5046715502701108\n",
      "Epoch Time Cost: 1224.85 s\n",
      " :::::: \n",
      "Testing\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.5209, Mean time to accident= 3.498, Adjusted AATC = 2.608\n",
      "Recall@80%, Time to accident = 4.003\n",
      "Recall@80%, Precision = 0.3994\n",
      " :::::: \n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/training/\n",
      "Average Precision= 0.6906 ,mean Time to accident= 3.39\n",
      "Recall@80%, Time to accident= 3.65\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.3912 ,mean Time to accident= 3.858\n",
      "Recall@80%, Time to accident= 4.116\n",
      "Epoch: 92  done. Loss: 0.49247943479863426\n",
      "Epoch Time Cost: 1242.32 s\n",
      " :::::: \n",
      "Testing\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.5279, Mean time to accident= 3.547, Adjusted AATC = 2.633\n",
      "Recall@80%, Time to accident = 4.119\n",
      "Recall@80%, Precision = 0.392\n",
      " :::::: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4b0860fcef3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#  print(file_name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m          \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m          \u001b[0mbatch_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#*0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m          \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 return format.read_array(bytes,\n\u001b[1;32m    261\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                                          pickle_kwargs=self.pickle_kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    758\u001b[0m                     \u001b[0mread_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_read_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m                     \u001b[0mread_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_count\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m                     array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[1;32m    762\u001b[0m                                                              count=read_count)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# done about that.  note that regular files can't be non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    940\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mZIP_STORED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m_read2\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    727\u001b[0m                         \"Close the writing handle before trying to read.\")\n\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# approx_ATTC = 2.56565\n",
    "\n",
    "saved_epoch = 86\n",
    "\n",
    "num1 = (126, 46)\n",
    "modes = ('training', 'testing')\n",
    "\n",
    "approx_ATTC = 3.282\n",
    "for epoch in range(saved_epoch, n_epochs):\n",
    "    #  epoch = epoch \n",
    "     # random chose batch.npz\n",
    "     epoch_loss = np.zeros((train_num,1),dtype = float)\n",
    "     n_batchs = np.arange(1,train_num+1)\n",
    "     np.random.shuffle(n_batchs)\n",
    "     tStart_epoch = time.time()\n",
    "    #  if epoch < 5:\n",
    "    #  if True:\n",
    "    #      approx_ATTC = 0\n",
    "\n",
    "     count_batch = 0  \n",
    "     start_time = time.time()\n",
    "     for batch in n_batchs:\n",
    "         count_batch += 1       \n",
    "\n",
    "         file_name = '%03d' %batch\n",
    "         batch_str = 'batch_'+file_name+'.npz'\n",
    "        #  print(file_name)\n",
    "         batch_data = np.load(train_path+batch_str, allow_pickle = True)\n",
    "         batch_xs = batch_data['data']  #*0\n",
    "         batch_ys = batch_data['labels']\n",
    "\n",
    "        #  df_paths = retrieve_dir_custom(new_save_path, batch_data, 'training', train_num, split_type=2)\n",
    "        #  phys_feat, feat_ind = retrieve_global_coord(df_paths, train_path, batch_str, train_num, mode='training')\n",
    "         phys_dict = np.load(new_save_path+'training/'+batch_str)\n",
    "         phys_feat, feat_ind = phys_dict['phys_feat'], phys_dict['feat_ind'] \n",
    "        #  phys_feat[:,:,:,[3,4,9,10,11,12]] = np.clip(phys_feat[:,:,:,[3,4,9,10,11,12]],-5,5)\n",
    "\n",
    "\n",
    "         if np.sum(np.isnan(phys_feat)) != 0:\n",
    "            print('NaNs in Physical Features')\n",
    "            print(np.sum(np.isnan(phys_feat)))\n",
    "\n",
    "\n",
    "         optimizer.zero_grad()\n",
    "         \n",
    "         if torch.cuda.is_available():\n",
    "            batch_loss, all_alphas, soft_pred = run_batch(torch.from_numpy(batch_xs).float().cuda(), phys_feat, feat_ind, torch.from_numpy(batch_ys).float().cuda(), model1, lstm, linear_pred, approx_ATTC, True)\n",
    "         else:\n",
    "            batch_loss, all_alphas, soft_pred = run_batch(torch.from_numpy(batch_xs).float(),  phys_feat, feat_ind, torch.from_numpy(batch_ys).float(), model1, lstm, linear_pred, approx_ATTC, True)\n",
    "\n",
    "         batch_loss.backward()\n",
    "         optimizer.step()\n",
    "         ## update steps\n",
    "         epoch_loss[batch-1] = batch_loss.detach().cpu().numpy()/batch_size\n",
    "          \n",
    "         name = 'WC_1_'+str(epoch)\n",
    "          \n",
    "     #  if torch.cuda.is_available():\n",
    "     #     approx_ATTC = test_all(train_num,train_path, phys_path_trn,model1, lstm, linear_pred, approx_ATTC, name, debug = True)\n",
    "     #  else:\n",
    "     #  approx_ATTC = test_all(test_num,test_path, new_save_path, ret_world_func, model1, lstm, linear_pred, approx_ATTC, name, 'testing', debug = True, log_file = '')\n",
    "     #  if epoch > 5:\n",
    "     approx_ATTC = test_all(train_num,train_path, new_save_path, new_save_path+'training/', model1, lstm, linear_pred, approx_ATTC, name, 'training', debug = True, log_file = '')\n",
    "     approx_ATTC = test_all(test_num,test_path, new_save_path, new_save_path+'testing/', model1, lstm, linear_pred, approx_ATTC, name, 'testing', debug = True, log_file = '')\n",
    "     #  approx_ATTC = 0  ##!!!!!!!!\n",
    "        \n",
    "     # print one epoch\n",
    "     print( \"Epoch:\", epoch+1, \" done. Loss:\", np.mean(epoch_loss) )\n",
    "     tStop_epoch = time.time()\n",
    "     print( \"Epoch Time Cost:\", round(tStop_epoch - tStart_epoch,2), \"s\" )\n",
    "     sys.stdout.flush()\n",
    "     if True:\n",
    "        model_name1, model_name2, model_name3 = 'model1_'+str(epoch), 'model2_'+str(epoch), 'model3_'+str(epoch)\n",
    "        torch.save(model1.state_dict(),save_path + model_name1 + '.pth')\n",
    "        torch.save(lstm.state_dict(),save_path + model_name2 + '.pth')\n",
    "        torch.save(linear_pred.state_dict(),save_path + model_name3 + '.pth')\n",
    "        print(' :::::: ')\n",
    "        # if (epoch+1) % 10  == 0:\n",
    "        #   print (\"Training\")\n",
    "        #   #test_all(num,path,model1,model2,model3, ATTC, name, debug = False)\n",
    "        #   test_all(train_num,train_path, new_save_path, new_save_path+'training/',model1, lstm, linear_pred, approx_ATTC, name, 'training', debug = False, log_file = '')\n",
    "        print (\"Testing\")\n",
    "        test_all(test_num,test_path, new_save_path, new_save_path+'testing/', model1, lstm, linear_pred, approx_ATTC, name, 'testing', debug = False, log_file = '')\n",
    "        print(' :::::: ')\n",
    "\n",
    "# epoch_loss[batch-1] = batch_loss.detach().cpu().numpy()/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZfNr7uvBjnR"
   },
   "source": [
    "Training Loop, Saving Every 15 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2057075,
     "status": "error",
     "timestamp": 1581729666731,
     "user": {
      "displayName": "ALEXANDER SAFONOV",
      "photoUrl": "",
      "userId": "06108573154481941124"
     },
     "user_tz": 480
    },
    "id": "LnH55ZRgfulL",
    "outputId": "7543d27b-ed23-419c-8255-c18f83e16d42"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5a1e400d5b60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m#   batch_xs = np.zeros(batch_xs.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m          \u001b[0mbatch_data_phys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphys_path_trn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'batch_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.npz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m          \u001b[0;31m### shuffle last dimension in batch dimension, to reduce dependency on agent_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m          \u001b[0mshuffle_phys_bool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'phys_path_trn' is not defined"
     ]
    }
   ],
   "source": [
    "import time, datetime\n",
    "# print('time elapsed from start ',datetime.timedelta(seconds=time.time()-start_time))\n",
    "from pytz import timezone\n",
    "\n",
    "# print('time elapsed from start ',datetime.timedelta(seconds=time.time()-start_time))\n",
    "\n",
    "log_file = os.path.join(save_path, 'log.txt')\n",
    "file2 = open(log_file, 'a')\n",
    "\n",
    "fmt = \"%Y-%m-%d %H:%M:%S %Z%z\"\n",
    "now_pacific = datetime.datetime.now(timezone('US/Pacific'))\n",
    "now_pacific = now_pacific.strftime(fmt)\n",
    "\n",
    "file2.write(\"\\n\" + now_pacific)\n",
    "file2.close()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "### move window_length to better location\n",
    "window_length = 20\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "#      epoch = epoch #+ 80\n",
    "     # random chose batch.npz\n",
    "     epoch_loss = np.zeros((train_num,1),dtype = float)\n",
    "     n_batchs = np.arange(1,train_num+1)\n",
    "     np.random.shuffle(n_batchs)\n",
    "     tStart_epoch = time.time()\n",
    "     if epoch == 0:\n",
    "         approx_ATTC = 0\n",
    "     for batch in n_batchs:\n",
    "         file_name = '%03d' %batch\n",
    "         batch_data = np.load(train_path+'batch_'+file_name+'.npz', allow_pickle = True)\n",
    "         batch_xs = batch_data['data']\n",
    "         batch_ys = batch_data['labels']\n",
    "        #  if epoch < 15:\n",
    "        #   batch_xs = np.zeros(batch_xs.shape)\n",
    "          \n",
    "         batch_data_phys = np.load(phys_path_trn+'batch_'+file_name+'.npz', allow_pickle = True)['data']\n",
    "         ### shuffle last dimension in batch dimension, to reduce dependency on agent_id\n",
    "         shuffle_phys_bool = True\n",
    "         if shuffle_phys_bool:\n",
    "           batch_data_phys = shuffle_ind(batch_data_phys) \n",
    "\n",
    "         batch_odom = retrieve_odometry_batch(batch_data['labels'], batch_data['ID'], 'training')\n",
    "          \n",
    "\n",
    "         ## retrieve batch_odometry\n",
    "\n",
    "         optimizer.zero_grad()\n",
    "         \n",
    "         if torch.cuda.is_available():\n",
    "            batch_loss, all_alphas, soft_pred = run_batch(torch.from_numpy(batch_xs).float().cuda(), torch.from_numpy(batch_data_phys).float().cuda(), torch.from_numpy(batch_odom).float().cuda(), torch.from_numpy(batch_ys).float().cuda(), model1, lstm, linear_pred, approx_ATTC, window_length, True)\n",
    "         else:\n",
    "            batch_loss, all_alphas, soft_pred = run_batch(torch.from_numpy(batch_xs).float(),torch.from_numpy(batch_data_phys).float(), torch.from_numpy(batch_odom).float() , torch.from_numpy(batch_ys).float(),model1, lstm, linear_pred, np.float(approx_ATTC), window_length, True)\n",
    "         \n",
    "         batch_loss.backward()\n",
    "         optimizer.step()\n",
    "         ## update steps\n",
    "         epoch_loss[batch-1] = batch_loss.detach().cpu().numpy()/batch_size\n",
    "          \n",
    "         name = 'Perfect_Future_FC_'+str(epoch)\n",
    "          \n",
    "     if torch.cuda.is_available():\n",
    "        approx_ATTC = test_all(train_num,train_path, phys_path_trn, 'training',model1, lstm, linear_pred, approx_ATTC, window_length, name, debug = True, log_file = log_file)\n",
    "     else:\n",
    "        approx_ATTC = test_all(train_num,train_path, phys_path_trn, 'training',model1, lstm, linear_pred, approx_ATTC, window_length, name, debug = True, log_file = log_file)\n",
    "\n",
    "     tStop_epoch = time.time()\n",
    "    \n",
    "\n",
    "\n",
    "     \n",
    "     # print( \"Epoch Time Cost:\", round(tStop_epoch - tStart_epoch,2), \"s\" )\n",
    "     print('Epoch Time Cost ',datetime.timedelta(seconds=tStop_epoch - tStart_epoch))\n",
    "     print('Time elapsed from start ',datetime.timedelta(seconds=time.time()-start_time))\n",
    "     sys.stdout.flush()\n",
    "     if (epoch+1) % 5  == 0:\n",
    "        print('saving model\\n')\n",
    "\n",
    "        torch.save({\n",
    "          'epoch': epoch,\n",
    "          'model1_state_dict': model1.state_dict(),\n",
    "          'lstm_state_dict': lstm.state_dict(),\n",
    "          'linear_pred_state_dict': linear_pred.state_dict(),\n",
    "          'optimizer_state_dict': optimizer.state_dict(),\n",
    "          'loss_history': loss_history, \n",
    "          'readme': 'Adding Physical Features to Local Appearance Features'\n",
    "          }, os.path.join(save_path, 'chkpt-' + str(epoch) + '.tar'))\n",
    "        \n",
    "        \n",
    "        ## retrieve odometry\n",
    "        print(' :::::: ')\n",
    "        print(' :::::: ')\n",
    "        # print (\"Training\")\n",
    "        # file2 = open(log_file, 'a')\n",
    "        # file2.write(\"training\\n\")\n",
    "        # file2.close()\n",
    "        # test_all(train_num,train_path, phys_path_trn, 'training',model1, lstm, linear_pred, approx_ATTC, window_length, name, debug = True, log_file = log_file)\n",
    "        print (\"Testing\")\n",
    "        file2 = open(log_file, 'a')\n",
    "        file2.write(\"testing\\n\")\n",
    "        file2.close()\n",
    "        test_all(test_num,test_path, phys_path_tst, 'testing',model1, lstm, linear_pred, approx_ATTC, window_length, name, debug = True, log_file = log_file)\n",
    "        print(' :::::: ')\n",
    "        print(' :::::: ')\n",
    "     else:\n",
    "        print (\"Testing\")\n",
    "        file2 = open(log_file, 'a')\n",
    "        file2.write(\"testing\\n\")\n",
    "        file2.close()\n",
    "        test_all(test_num,test_path, phys_path_tst, 'testing',model1, lstm, linear_pred, approx_ATTC, window_length, name, debug = True, log_file = log_file)\n",
    "\n",
    "     # print one epoch\n",
    "     str3 = \"Epoch:\" + str(epoch+1) + \" done. Loss:\" + str(np.mean(epoch_loss))\n",
    "     print( str3 )\n",
    "    \n",
    "     print( \"Epoch Time Cost:\", round(tStop_epoch - tStart_epoch,2), \"s\" )\n",
    "     sys.stdout.flush()\n",
    "\n",
    "     loss_history.append(np.mean(epoch_loss))\n",
    "     file2 = open(log_file, 'a')\n",
    "     file2.write(str3 + \"\\n\")\n",
    "     file2.close()\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "WC_appearance_4_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
