{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UrKWmxee-nt0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "%cd /content/gdrive/My\\ Drive/Accident_Anticipation/depth_estimation/google_distance_code/google-research/depth_from_video_in_the_wild\n",
    "from image_utils import *\n",
    "\n",
    "%cd /content/gdrive/My\\ Drive/Accident_Anticipation/Replicated_Papers/accident_scripts/\n",
    "cur_dir = %pwd\n",
    "sys.path.append(cur_dir)\n",
    "\n",
    "%cd /content/gdrive/My\\ Drive/Accident_Anticipation/SASHA_NOTEBOOKS/py_files/\n",
    "cur_dir = %pwd\n",
    "sys.path.append(cur_dir)\n",
    "\n",
    "%cd /content/gdrive/My\\ Drive/Accident_Anticipation/Replicated_Papers/Relevant_Code/QRNN/pytorch-qrnn/torchqrnn\n",
    "cur_dir = %pwd\n",
    "sys.path.append(cur_dir)\n",
    "\n",
    "%cd /content/gdrive/My\\ Drive/Accident_Anticipation/Anticipating-Accidents/\n",
    "cur_dir = %pwd\n",
    "sys.path.append(cur_dir)\n",
    "\n",
    "nb_path = \"/content/gdrive/My Drive/Accident_Anticipation/libraries\"\n",
    "# !pip install --target=$nb_path cupy pynvrtc\n",
    "sys.path.insert(0, nb_path)\n",
    "\n",
    "\n",
    "# warnings.filterwarnings(action='once')\n",
    "warnings.filterwarnings('ignore')\n",
    "# ! pip install cupy pynvrtc\n",
    "\n",
    "\n",
    "#### Attention Learning Blogs\n",
    "# Pytorch attention implementations\n",
    "# https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1295,
     "status": "error",
     "timestamp": 1583304342571,
     "user": {
      "displayName": "ALEXANDER SAFONOV",
      "photoUrl": "",
      "userId": "06108573154481941124"
     },
     "user_tz": 480
    },
    "id": "s7n0HcaPT_YE",
    "outputId": "72ad0b94-6703-454c-a4b8-4d8acfd29675"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-454a70a04629>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpermutations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0maccident_physical_fgsm_lighter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0;31m#_2 as evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# from qrnn import QRNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'accident_physical_fgsm_lighter'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pdb\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from itertools import permutations\n",
    "from accident_physical_fgsm_lighter import evaluation #_2 as evaluation\n",
    "# from qrnn import QRNN\n",
    "import pickle \n",
    "import pandas as pd\n",
    "from qrnn import QRNN\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "dir_str = '/content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents'\n",
    "\n",
    "# path\n",
    "train_path = dir_str + '/dataset/features/training/' #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Change this line\n",
    "test_path = dir_str +'/dataset/features/testing/'   #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Change this line\n",
    "demo_path = dir_str +'/dataset/features/testing/'   #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Change this line\n",
    "\n",
    "new_save_path = '/content/gdrive/My Drive/Accident_Anticipation/EXTRACT_PHYSICS_NOTEBOOKS/physical_features/5_world_coordinates_5npz/'\n",
    "\n",
    "default_model_path = './model/demo_model'   #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Change this line\n",
    "save_path = './model/SASHA/WC_appearance_4_1/'  #+ dir_str +'/'                   #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Change this line\n",
    "video_path = './dataset/videos/testing/positive/'\n",
    "\n",
    "# batch_number\n",
    "train_num = 126\n",
    "test_num = 46\n",
    "\n",
    "\n",
    "############## Train Parameters #################\n",
    "# Parameters\n",
    "learning_rate = 0.0001\n",
    "n_epochs = 110\n",
    "batch_size = 10\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 4096 # fc6 or fc7(1*4096)          #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Change this line\n",
    "n_detection = 20 # number of object of each image (include image features)\n",
    "\n",
    "n_phys_hidden = 50\n",
    "phys_dim = 50\n",
    "\n",
    "# n_img_hidden = 256+ phys_dim # embedding image features \n",
    "# n_att_hidden = 256+ phys_dim # embedding object features\n",
    "n_img_hidden = 200 #20 + phys_dim # embedding image features \n",
    "n_att_hidden = 200 #20 + phys_dim # embedding object features\n",
    "\n",
    "n_classes = 2 # has accident or not\n",
    "n_frames = 100 # number of frame in each video \n",
    "\n",
    "\n",
    "# n_hidden =  512 +2*phys_dim  #+ 150 # hidden layer num of LSTM\n",
    "n_hidden = n_img_hidden+n_att_hidden+n_phys_hidden*2 #300 #+25*2\n",
    "\n",
    "p_drop = 0.5\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "root = '/content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents'\n",
    "# path\n",
    "train_path = root + '/dataset/features/training/' \n",
    "test_path = root +  '/dataset/features/testing/' \n",
    "\n",
    "# new_save_path = '/content/gdrive/My Drive/Accident_Anticipation/EXTRACT_PHYSICS_NOTEBOOKS/physical_features/5_world_coordinates_3v/'\n",
    "# path_glob = '/content/gdrive/My Drive/Accident_Anticipation/EXTRACT_PHYSICS_NOTEBOOKS/physical_features/5_world_coordinates_2/'\n",
    "\n",
    "num1 = (126, 46)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wHFMG0FqUXRa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "## change function that we draw from\n",
    "from WC_appearance_4 import EmbedLayer, Attention_LSTM_2, Linear_Pred\n",
    "from WC_appearance_4 import soft_max_entropy_loss_logits, temp_loss, run_batch, approx_evaluation, test_all\n",
    "\n",
    "## add variables for hidden layers\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  model1 = EmbedLayer(n_input, n_img_hidden, n_att_hidden, n_hidden, n_phys_hidden, p_drop).cuda()\n",
    "  lstm = Attention_LSTM_2(batch_size,n_hidden,n_hidden, p_drop).cuda()\n",
    "  linear_pred = Linear_Pred(batch_size,n_hidden,n_classes).cuda()\n",
    "else:\n",
    "  model1 = EmbedLayer(n_input, n_img_hidden, n_att_hidden, n_hidden, n_phys_hidden, p_drop)\n",
    "  lstm = Attention_LSTM_2(batch_size,n_hidden,n_hidden, p_drop)\n",
    "  linear_pred = Linear_Pred(batch_size,n_hidden,n_classes)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(model1.parameters())+list(lstm.parameters())+list(linear_pred.parameters()), lr = 0.00005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7sJJk5tFje_K"
   },
   "outputs": [],
   "source": [
    "print(save_path)\n",
    "print(os.listdir(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2254,
     "status": "ok",
     "timestamp": 1582174325603,
     "user": {
      "displayName": "ALEXANDER SAFONOV",
      "photoUrl": "",
      "userId": "06108573154481941124"
     },
     "user_tz": 480
    },
    "id": "ooiDXyASyzsR",
    "outputId": "64bf3605-296e-487d-ab01-b7cb670e1f3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/SASHA/WC_appearance_4_1/\n",
      "['model1_85.pth', 'model1_84.pth', 'model3_84.pth', 'model2_84.pth', 'model3_85.pth', 'model1_88.pth', 'model3_87.pth', 'model2_85.pth', 'model1_87.pth', 'model2_88.pth', 'model1_86.pth', 'model2_87.pth', 'model3_86.pth', 'model2_86.pth', 'model3_88.pth']\n"
     ]
    }
   ],
   "source": [
    "print(save_path)\n",
    "print(os.listdir(save_path))\n",
    "len_dir = len(os.listdir(save_path))/3\n",
    "ind = (len_dir-1)*3\n",
    "tup = (ind, ind+1, ind+2)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    map_location=lambda storage, loc: storage.cuda()\n",
    "else:\n",
    "    map_location='cpu'\n",
    "\n",
    "map_location=torch.device('cpu')\n",
    "model1.load_state_dict(torch.load(save_path + 'model1_85.pth', map_location = map_location))\n",
    "lstm.load_state_dict(torch.load(save_path + 'model2_85.pth', map_location = map_location))\n",
    "linear_pred.load_state_dict(torch.load(save_path + 'model3_85.pth', map_location = map_location  ))\n",
    "\n",
    "saved_epoch = 86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSJtwIqtUmga"
   },
   "source": [
    "Training Loop, Saving Every 15 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7844687,
     "status": "error",
     "timestamp": 1582183990797,
     "user": {
      "displayName": "ALEXANDER SAFONOV",
      "photoUrl": "",
      "userId": "06108573154481941124"
     },
     "user_tz": 480
    },
    "id": "Qso7r3nsUh6c",
    "outputId": "70d9b956-aa83-4d1f-f3b4-8ac3bed861df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/training/\n",
      "Average Precision= 0.7162 ,mean Time to accident= 3.108\n",
      "Recall@80%, Time to accident= 3.356\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.4128 ,mean Time to accident= 3.521\n",
      "Recall@80%, Time to accident= 3.697\n",
      "Epoch: 87  done. Loss: 0.5294804654187626\n",
      "Epoch Time Cost: 1191.74 s\n",
      " :::::: \n",
      "Testing\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.5435, Mean time to accident= 3.233, Adjusted AATC = 2.403\n",
      "Recall@80%, Time to accident = 3.699\n",
      "Recall@80%, Precision = 0.4045\n",
      " :::::: \n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/training/\n",
      "Average Precision= 0.7314 ,mean Time to accident= 3.143\n",
      "Recall@80%, Time to accident= 3.397\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.4169 ,mean Time to accident= 3.695\n",
      "Recall@80%, Time to accident= 4.056\n",
      "Epoch: 88  done. Loss: 0.5304419079470256\n",
      "Epoch Time Cost: 1242.82 s\n",
      " :::::: \n",
      "Testing\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.5497, Mean time to accident= 3.432, Adjusted AATC = 2.561\n",
      "Recall@80%, Time to accident = 4.065\n",
      "Recall@80%, Precision = 0.3969\n",
      " :::::: \n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/training/\n",
      "Average Precision= 0.7403 ,mean Time to accident= 3.271\n",
      "Recall@80%, Time to accident= 3.528\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.4117 ,mean Time to accident= 3.694\n",
      "Recall@80%, Time to accident= 3.793\n",
      "Epoch: 89  done. Loss: 0.9130627098537626\n",
      "Epoch Time Cost: 1240.82 s\n",
      " :::::: \n",
      "Testing\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.5389, Mean time to accident= 3.325, Adjusted AATC = 2.461\n",
      "Recall@80%, Time to accident = 3.794\n",
      "Recall@80%, Precision = 0.4058\n",
      " :::::: \n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/training/\n",
      "Average Precision= 0.6992 ,mean Time to accident= 3.385\n",
      "Recall@80%, Time to accident= 3.74\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.4052 ,mean Time to accident= 3.715\n",
      "Recall@80%, Time to accident= 3.968\n",
      "Epoch: 90  done. Loss: 0.5823283107980847\n",
      "Epoch Time Cost: 1216.34 s\n",
      " :::::: \n",
      "Testing\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.5344, Mean time to accident= 3.448, Adjusted AATC = 2.547\n",
      "Recall@80%, Time to accident = 3.969\n",
      "Recall@80%, Precision = 0.4058\n",
      " :::::: \n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/training/\n",
      "Average Precision= 0.6700 ,mean Time to accident= 3.303\n",
      "Recall@80%, Time to accident= 1.94\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.3901 ,mean Time to accident= 3.784\n",
      "Recall@80%, Time to accident= 3.991\n",
      "Epoch: 91  done. Loss: 0.5046715502701108\n",
      "Epoch Time Cost: 1224.85 s\n",
      " :::::: \n",
      "Testing\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.5209, Mean time to accident= 3.498, Adjusted AATC = 2.608\n",
      "Recall@80%, Time to accident = 4.003\n",
      "Recall@80%, Precision = 0.3994\n",
      " :::::: \n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/training/\n",
      "Average Precision= 0.6906 ,mean Time to accident= 3.39\n",
      "Recall@80%, Time to accident= 3.65\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.3912 ,mean Time to accident= 3.858\n",
      "Recall@80%, Time to accident= 4.116\n",
      "Epoch: 92  done. Loss: 0.49247943479863426\n",
      "Epoch Time Cost: 1242.32 s\n",
      " :::::: \n",
      "Testing\n",
      "Using features from path: /content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents/dataset/features/testing/\n",
      "Average Precision= 0.5279, Mean time to accident= 3.547, Adjusted AATC = 2.633\n",
      "Recall@80%, Time to accident = 4.119\n",
      "Recall@80%, Precision = 0.392\n",
      " :::::: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4b0860fcef3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#  print(file_name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m          \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m          \u001b[0mbatch_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#*0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m          \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 return format.read_array(bytes,\n\u001b[1;32m    261\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                                          pickle_kwargs=self.pickle_kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    758\u001b[0m                     \u001b[0mread_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_read_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m                     \u001b[0mread_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_count\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m                     array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[1;32m    762\u001b[0m                                                              count=read_count)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# done about that.  note that regular files can't be non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    940\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mZIP_STORED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m_read2\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    727\u001b[0m                         \"Close the writing handle before trying to read.\")\n\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# approx_ATTC = 2.56565\n",
    "\n",
    "saved_epoch = 86\n",
    "\n",
    "num1 = (126, 46)\n",
    "modes = ('training', 'testing')\n",
    "\n",
    "approx_ATTC = 3.282\n",
    "for epoch in range(saved_epoch, n_epochs):\n",
    "    #  epoch = epoch \n",
    "     # random chose batch.npz\n",
    "     epoch_loss = np.zeros((train_num,1),dtype = float)\n",
    "     n_batchs = np.arange(1,train_num+1)\n",
    "     np.random.shuffle(n_batchs)\n",
    "     tStart_epoch = time.time()\n",
    "    #  if epoch < 5:\n",
    "    #  if True:\n",
    "    #      approx_ATTC = 0\n",
    "\n",
    "     count_batch = 0  \n",
    "     start_time = time.time()\n",
    "     for batch in n_batchs:\n",
    "         count_batch += 1       \n",
    "\n",
    "         file_name = '%03d' %batch\n",
    "         batch_str = 'batch_'+file_name+'.npz'\n",
    "        #  print(file_name)\n",
    "         batch_data = np.load(train_path+batch_str, allow_pickle = True)\n",
    "         batch_xs = batch_data['data']  #*0\n",
    "         batch_ys = batch_data['labels']\n",
    "\n",
    "        #  df_paths = retrieve_dir_custom(new_save_path, batch_data, 'training', train_num, split_type=2)\n",
    "        #  phys_feat, feat_ind = retrieve_global_coord(df_paths, train_path, batch_str, train_num, mode='training')\n",
    "         phys_dict = np.load(new_save_path+'training/'+batch_str)\n",
    "         phys_feat, feat_ind = phys_dict['phys_feat'], phys_dict['feat_ind'] \n",
    "        #  phys_feat[:,:,:,[3,4,9,10,11,12]] = np.clip(phys_feat[:,:,:,[3,4,9,10,11,12]],-5,5)\n",
    "\n",
    "\n",
    "         if np.sum(np.isnan(phys_feat)) != 0:\n",
    "            print('NaNs in Physical Features')\n",
    "            print(np.sum(np.isnan(phys_feat)))\n",
    "\n",
    "\n",
    "         optimizer.zero_grad()\n",
    "         \n",
    "         if torch.cuda.is_available():\n",
    "            batch_loss, all_alphas, soft_pred = run_batch(torch.from_numpy(batch_xs).float().cuda(), phys_feat, feat_ind, torch.from_numpy(batch_ys).float().cuda(), model1, lstm, linear_pred, approx_ATTC, True)\n",
    "         else:\n",
    "            batch_loss, all_alphas, soft_pred = run_batch(torch.from_numpy(batch_xs).float(),  phys_feat, feat_ind, torch.from_numpy(batch_ys).float(), model1, lstm, linear_pred, approx_ATTC, True)\n",
    "\n",
    "         batch_loss.backward()\n",
    "         optimizer.step()\n",
    "         ## update steps\n",
    "         epoch_loss[batch-1] = batch_loss.detach().cpu().numpy()/batch_size\n",
    "          \n",
    "         name = 'WC_1_'+str(epoch)\n",
    "          \n",
    "     #  if torch.cuda.is_available():\n",
    "     #     approx_ATTC = test_all(train_num,train_path, phys_path_trn,model1, lstm, linear_pred, approx_ATTC, name, debug = True)\n",
    "     #  else:\n",
    "     #  approx_ATTC = test_all(test_num,test_path, new_save_path, ret_world_func, model1, lstm, linear_pred, approx_ATTC, name, 'testing', debug = True, log_file = '')\n",
    "     #  if epoch > 5:\n",
    "     approx_ATTC = test_all(train_num,train_path, new_save_path, new_save_path+'training/', model1, lstm, linear_pred, approx_ATTC, name, 'training', debug = True, log_file = '')\n",
    "     approx_ATTC = test_all(test_num,test_path, new_save_path, new_save_path+'testing/', model1, lstm, linear_pred, approx_ATTC, name, 'testing', debug = True, log_file = '')\n",
    "     #  approx_ATTC = 0  ##!!!!!!!!\n",
    "        \n",
    "     # print one epoch\n",
    "     print( \"Epoch:\", epoch+1, \" done. Loss:\", np.mean(epoch_loss) )\n",
    "     tStop_epoch = time.time()\n",
    "     print( \"Epoch Time Cost:\", round(tStop_epoch - tStart_epoch,2), \"s\" )\n",
    "     sys.stdout.flush()\n",
    "     if True:\n",
    "    #  if True:\n",
    "        model_name1, model_name2, model_name3 = 'model1_'+str(epoch), 'model2_'+str(epoch), 'model3_'+str(epoch)\n",
    "        torch.save(model1.state_dict(),save_path + model_name1 + '.pth')\n",
    "        torch.save(lstm.state_dict(),save_path + model_name2 + '.pth')\n",
    "        torch.save(linear_pred.state_dict(),save_path + model_name3 + '.pth')\n",
    "        print(' :::::: ')\n",
    "        # if (epoch+1) % 10  == 0:\n",
    "        #   print (\"Training\")\n",
    "        #   #test_all(num,path,model1,model2,model3, ATTC, name, debug = False)\n",
    "        #   test_all(train_num,train_path, new_save_path, new_save_path+'training/',model1, lstm, linear_pred, approx_ATTC, name, 'training', debug = False, log_file = '')\n",
    "        print (\"Testing\")\n",
    "        test_all(test_num,test_path, new_save_path, new_save_path+'testing/', model1, lstm, linear_pred, approx_ATTC, name, 'testing', debug = False, log_file = '')\n",
    "        print(' :::::: ')\n",
    "\n",
    "## epoch is shifted by five\n",
    "\n",
    "# epoch_loss[batch-1] = batch_loss.detach().cpu().numpy()/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KH-63y6YNJBu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2320,
     "status": "error",
     "timestamp": 1580197319048,
     "user": {
      "displayName": "ALEXANDER SAFONOV",
      "photoUrl": "",
      "userId": "06108573154481941124"
     },
     "user_tz": 480
    },
    "id": "CcsXre_dAyJ2",
    "outputId": "3750b522-853a-4407-dd6d-8c146dcc5873"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d9f49144ed55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mload_pretrained\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m       \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# print(save_path)\n",
    "# print(os.listdir(save_path))\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#do change these\n",
    "load_pretrained = True\n",
    "saved_epoch = '79'\n",
    "\n",
    "#dont change these\n",
    "start_epoch = 0 # this gets modified depending on loaded model\n",
    "loss_history = []\n",
    "\n",
    "if load_pretrained:\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "      map_location=lambda storage, loc: storage.cuda()\n",
    "  else:\n",
    "      map_location='cpu'\n",
    "  print('map_location =', map_location)\n",
    "  # map_location=torch.device('cpu')\n",
    "\n",
    "  #model1.load_state_dict(torch.load(save_path + os.listdir(save_path)[-3], map_location = map_location))\n",
    "  #lstm.load_state_dict(torch.load(save_path + os.listdir(save_path)[-2], map_location = map_location))\n",
    "  #linear_pred.load_state_dict(torch.load(save_path + os.listdir(save_path)[-1], map_location = map_location  ))\n",
    "\n",
    "  PATH = os.path.join(save_path, 'chkpt-' + str(saved_epoch) + '.tar')\n",
    "  print('loading in ', PATH)\n",
    "\n",
    "  checkpoint = torch.load(PATH)\n",
    "  model1.load_state_dict(checkpoint['model1_state_dict'])\n",
    "  lstm.load_state_dict(checkpoint['lstm_state_dict'])\n",
    "  linear_pred.load_state_dict(checkpoint['linear_pred_state_dict'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "  if 'loss_history' in checkpoint.keys():\n",
    "    loss_history = checkpoint['loss_history']\n",
    "\n",
    "  start_epoch = int(saved_epoch) + 1\n",
    "\n",
    "  #start = model_to_load.find('_')\n",
    "  #end = model_to_load.find('.')\n",
    "  #saved_epoch = model_to_load[start+1:end]\n",
    "  #start_epoch = int(saved_epoch) + 1 \n",
    "else:\n",
    "  start_epoch = 0\n",
    "\n",
    "\n",
    "print('start_epoch =', start_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aEoZ6B90BTr7"
   },
   "outputs": [],
   "source": [
    "def shuffle_ind(input_mat):\n",
    "  arr = np.arange(input_mat.shape[2])\n",
    "  np.random.shuffle(arr)\n",
    "  return(input_mat[:,:,arr,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e8_oQhAm9KFe"
   },
   "source": [
    "loading global coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_a5rbZ8k-CD7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "root = '/content/gdrive/My Drive/Accident_Anticipation/Anticipating-Accidents'\n",
    "# path\n",
    "train_path = root + '/dataset/features/training/' \n",
    "test_path = root +  '/dataset/features/testing/' \n",
    "\n",
    "new_save_path = '/content/gdrive/My Drive/Accident_Anticipation/EXTRACT_PHYSICS_NOTEBOOKS/physical_features/5_world_coordinates_w_velocity/'\n",
    "\n",
    "num1 = (126, 46)\n",
    "\n",
    "def retrieve_global_coord(global_df_path, feat_path, batch_str, num_samp, mode='training'):\n",
    "  glob_feat = np.zeros((10,100, 20, 4))\n",
    "  org_bb = np.load(feat_path+batch_str) # use org_bb to retrieve bounding boxes, and the correct video ID\n",
    "  frame_paths, frame_labels = retrieve_dir1(org_bb['labels'], org_bb['ID'], mode, num_samp)\n",
    "  global_df_paths = retrieve_dir_custom(new_save_path, org_bb, mode, num_samp, split_type=2)\n",
    " \n",
    "  for batch_num in range(10):\n",
    "    cur_df_path = global_df_paths[batch_num]\n",
    "    # add try except functionality in case df doesn't exisit\n",
    "    trajectory_df = pd.read_pickle(cur_df_path)\n",
    "    time_stamps = np.linspace(0,5,100) # use time_stamps to retrieve the correct index\n",
    "    frames = trajectory_df.groupby('TIMESTAMPS')\n",
    "\n",
    "    for group_name, group_data in frames:\n",
    "      time_ind = np.argwhere(time_stamps == group_name)[0][0] \n",
    "      cur_x, cur_y = group_data['X'], group_data['Z']\n",
    "      v_x, v_y = group_data['v_x'], group_data['v_y']\n",
    "      glob_feat[batch_num, time_ind,0:len(cur_x),0] = cur_x\n",
    "      glob_feat[batch_num, time_ind,0:len(cur_x),1] = cur_y\n",
    "      glob_feat[batch_num, time_ind,0:len(cur_x),2] = v_x\n",
    "      glob_feat[batch_num, time_ind,0:len(cur_x),3] = v_y\n",
    "\n",
    "  return glob_feat \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kbrsDboeJdbC"
   },
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# glob_feat = retrieve_global_coord(new_save_path, train_path, 'batch_014.npz', num1[0])\n",
    "\n",
    "# print(time.time() - start_time)\n",
    "# # => {10,100,20,4} => {10, 100, 191, 8} => {10, 100, 190, num_hidden_var} => {10, 100, 20, 45}\n",
    "\n",
    "# print(glob_feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k816YwMig4i1"
   },
   "source": [
    "Currently working on this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oa1M3RFLArNn"
   },
   "outputs": [],
   "source": [
    "batch_str = 'batch_005.npz'\n",
    "mode = 'training'\n",
    "vid_num = 2\n",
    "\n",
    "num1 = (126, 46)\n",
    "modes = ('training', 'testing')\n",
    "mode_paths = (train_path, test_path)\n",
    "\n",
    "# depth_np = np.load(depth_complete_paths[vid_num]) # ['distance_map']\n",
    "# odom_np = np.load(odom_complete_paths[vid_num]) # ['rot', 'trans', 'instrinsic_matrix', 'orientation', 'position']\n",
    "\n",
    "for mode_iter in range(2):\n",
    "  num_batches = num1[mode_iter]\n",
    "  mode = modes[mode_iter]\n",
    "\n",
    "  cur_path = optimal_crop_path+'/'+mode+'/'\n",
    "\n",
    "  for batch_iter in range(num_batches):\n",
    "    batch_str = 'batch_%03d.npz'%(batch_iter+1)\n",
    "    if batch_iter % 15 == 0:\n",
    "      print(batch_str)\n",
    "\n",
    "    # cropped_dict = np.load(cur_path+batch_str)\n",
    "    org_bb = np.load(mode_paths[mode_iter]+batch_str) # use org_bb to retrieve bounding boxes, and the correct video ID\n",
    "    resized_bb = np.load(cur_path+batch_str) # use org_bb to retrieve bounding boxes, and the correct video ID\n",
    "\n",
    "    bounding_box = np.load(optimal_crop_path+mode+'/'+batch_str)\n",
    "    frame_paths, frame_labels = retrieve_dir1(org_bb['labels'], org_bb['ID'],mode, num1[mode_iter])\n",
    "    df_paths = retrieve_dir_custom(path_glob, org_bb, mode, num1[mode_iter], split_type=2)\n",
    "\n",
    "    print(df_paths)\n",
    "    break\n",
    "    # add function that converts df to numpy array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkIsmrfk8AGS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZfNr7uvBjnR"
   },
   "source": [
    "Training Loop, Saving Every 15 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2057075,
     "status": "error",
     "timestamp": 1581729666731,
     "user": {
      "displayName": "ALEXANDER SAFONOV",
      "photoUrl": "",
      "userId": "06108573154481941124"
     },
     "user_tz": 480
    },
    "id": "LnH55ZRgfulL",
    "outputId": "7543d27b-ed23-419c-8255-c18f83e16d42"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5a1e400d5b60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m#   batch_xs = np.zeros(batch_xs.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m          \u001b[0mbatch_data_phys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphys_path_trn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'batch_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.npz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m          \u001b[0;31m### shuffle last dimension in batch dimension, to reduce dependency on agent_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m          \u001b[0mshuffle_phys_bool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'phys_path_trn' is not defined"
     ]
    }
   ],
   "source": [
    "import time, datetime\n",
    "# print('time elapsed from start ',datetime.timedelta(seconds=time.time()-start_time))\n",
    "from pytz import timezone\n",
    "\n",
    "# print('time elapsed from start ',datetime.timedelta(seconds=time.time()-start_time))\n",
    "\n",
    "log_file = os.path.join(save_path, 'log.txt')\n",
    "file2 = open(log_file, 'a')\n",
    "\n",
    "fmt = \"%Y-%m-%d %H:%M:%S %Z%z\"\n",
    "now_pacific = datetime.datetime.now(timezone('US/Pacific'))\n",
    "now_pacific = now_pacific.strftime(fmt)\n",
    "\n",
    "file2.write(\"\\n\" + now_pacific)\n",
    "file2.close()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "### move window_length to better location\n",
    "window_length = 20\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "     epoch = epoch #+ 80\n",
    "     \n",
    "     # random chose batch.npz\n",
    "     epoch_loss = np.zeros((train_num,1),dtype = float)\n",
    "     n_batchs = np.arange(1,train_num+1)\n",
    "     np.random.shuffle(n_batchs)\n",
    "     tStart_epoch = time.time()\n",
    "     if epoch == 0:\n",
    "         approx_ATTC = 0\n",
    "     for batch in n_batchs:\n",
    "         file_name = '%03d' %batch\n",
    "         batch_data = np.load(train_path+'batch_'+file_name+'.npz', allow_pickle = True)\n",
    "         batch_xs = batch_data['data']\n",
    "         batch_ys = batch_data['labels']\n",
    "        #  if epoch < 15:\n",
    "        #   batch_xs = np.zeros(batch_xs.shape)\n",
    "          \n",
    "         batch_data_phys = np.load(phys_path_trn+'batch_'+file_name+'.npz', allow_pickle = True)['data']\n",
    "         ### shuffle last dimension in batch dimension, to reduce dependency on agent_id\n",
    "         shuffle_phys_bool = True\n",
    "         if shuffle_phys_bool:\n",
    "           batch_data_phys = shuffle_ind(batch_data_phys) \n",
    "\n",
    "         batch_odom = retrieve_odometry_batch(batch_data['labels'], batch_data['ID'], 'training')\n",
    "          \n",
    "\n",
    "         ## retrieve batch_odometry\n",
    "\n",
    "         optimizer.zero_grad()\n",
    "         \n",
    "         if torch.cuda.is_available():\n",
    "            batch_loss, all_alphas, soft_pred = run_batch(torch.from_numpy(batch_xs).float().cuda(), torch.from_numpy(batch_data_phys).float().cuda(), torch.from_numpy(batch_odom).float().cuda(), torch.from_numpy(batch_ys).float().cuda(), model1, lstm, linear_pred, approx_ATTC, window_length, True)\n",
    "         else:\n",
    "            batch_loss, all_alphas, soft_pred = run_batch(torch.from_numpy(batch_xs).float(),torch.from_numpy(batch_data_phys).float(), torch.from_numpy(batch_odom).float() , torch.from_numpy(batch_ys).float(),model1, lstm, linear_pred, np.float(approx_ATTC), window_length, True)\n",
    "         \n",
    "         batch_loss.backward()\n",
    "         optimizer.step()\n",
    "         ## update steps\n",
    "         epoch_loss[batch-1] = batch_loss.detach().cpu().numpy()/batch_size\n",
    "          \n",
    "         name = 'Perfect_Future_FC_'+str(epoch)\n",
    "          \n",
    "     if torch.cuda.is_available():\n",
    "        approx_ATTC = test_all(train_num,train_path, phys_path_trn, 'training',model1, lstm, linear_pred, approx_ATTC, window_length, name, debug = True, log_file = log_file)\n",
    "     else:\n",
    "        approx_ATTC = test_all(train_num,train_path, phys_path_trn, 'training',model1, lstm, linear_pred, approx_ATTC, window_length, name, debug = True, log_file = log_file)\n",
    "\n",
    "     tStop_epoch = time.time()\n",
    "    \n",
    "\n",
    "\n",
    "     \n",
    "     # print( \"Epoch Time Cost:\", round(tStop_epoch - tStart_epoch,2), \"s\" )\n",
    "     print('Epoch Time Cost ',datetime.timedelta(seconds=tStop_epoch - tStart_epoch))\n",
    "     print('Time elapsed from start ',datetime.timedelta(seconds=time.time()-start_time))\n",
    "     sys.stdout.flush()\n",
    "     if (epoch+1) % 5  == 0:\n",
    "        print('saving model\\n')\n",
    "\n",
    "        torch.save({\n",
    "          'epoch': epoch,\n",
    "          'model1_state_dict': model1.state_dict(),\n",
    "          'lstm_state_dict': lstm.state_dict(),\n",
    "          'linear_pred_state_dict': linear_pred.state_dict(),\n",
    "          'optimizer_state_dict': optimizer.state_dict(),\n",
    "          'loss_history': loss_history, \n",
    "          'readme': 'Adding Physical Features to Local Appearance Features'\n",
    "          }, os.path.join(save_path, 'chkpt-' + str(epoch) + '.tar'))\n",
    "        \n",
    "        \n",
    "        ## retrieve odometry\n",
    "        print(' :::::: ')\n",
    "        print(' :::::: ')\n",
    "        # print (\"Training\")\n",
    "        # file2 = open(log_file, 'a')\n",
    "        # file2.write(\"training\\n\")\n",
    "        # file2.close()\n",
    "        # test_all(train_num,train_path, phys_path_trn, 'training',model1, lstm, linear_pred, approx_ATTC, window_length, name, debug = True, log_file = log_file)\n",
    "        print (\"Testing\")\n",
    "        file2 = open(log_file, 'a')\n",
    "        file2.write(\"testing\\n\")\n",
    "        file2.close()\n",
    "        test_all(test_num,test_path, phys_path_tst, 'testing',model1, lstm, linear_pred, approx_ATTC, window_length, name, debug = True, log_file = log_file)\n",
    "        print(' :::::: ')\n",
    "        print(' :::::: ')\n",
    "     else:\n",
    "        print (\"Testing\")\n",
    "        file2 = open(log_file, 'a')\n",
    "        file2.write(\"testing\\n\")\n",
    "        file2.close()\n",
    "        test_all(test_num,test_path, phys_path_tst, 'testing',model1, lstm, linear_pred, approx_ATTC, window_length, name, debug = True, log_file = log_file)\n",
    "\n",
    "     # print one epoch\n",
    "     str3 = \"Epoch:\" + str(epoch+1) + \" done. Loss:\" + str(np.mean(epoch_loss))\n",
    "     print( str3 )\n",
    "    \n",
    "     print( \"Epoch Time Cost:\", round(tStop_epoch - tStart_epoch,2), \"s\" )\n",
    "     sys.stdout.flush()\n",
    "\n",
    "     loss_history.append(np.mean(epoch_loss))\n",
    "     file2 = open(log_file, 'a')\n",
    "     file2.write(str3 + \"\\n\")\n",
    "     file2.close()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MnNE6h5XVbtR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3406470,
     "status": "error",
     "timestamp": 1572836340420,
     "user": {
      "displayName": "ALEXANDER SAFONOV",
      "photoUrl": "",
      "userId": "06108573154481941124"
     },
     "user_tz": 480
    },
    "id": "_IpHw-WZim0s",
    "outputId": "7bf2e67b-d680-40a8-e7fe-c81c6cd251d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map_location = <function <lambda> at 0x7f6c64196b70>\n",
      "loading in  ./model/SASHA/FC_True_Future/chkpt-24.tar\n",
      "start_epoch = 25\n",
      "Using features from path: ./dataset/features/testing/\n",
      "Average Precision= 0.6921, Mean time to accident= 1.547, Adjusted AATC = 1.029\n",
      "Recall@80%, Time to accident = 2.227\n",
      "Recall@80%, Precision = 0.4847\n",
      "map_location = <function <lambda> at 0x7f6c43d16048>\n",
      "loading in  ./model/SASHA/FC_True_Future/chkpt-29.tar\n",
      "start_epoch = 30\n",
      "Using features from path: ./dataset/features/testing/\n",
      "Average Precision= 0.6517, Mean time to accident= 1.799, Adjusted AATC = 1.213\n",
      "Recall@80%, Time to accident = 2.404\n",
      "Recall@80%, Precision = 0.4704\n",
      "map_location = <function <lambda> at 0x7f6c64196f28>\n",
      "loading in  ./model/SASHA/FC_True_Future/chkpt-34.tar\n",
      "start_epoch = 35\n",
      "Using features from path: ./dataset/features/testing/\n",
      "Average Precision= 0.6650, Mean time to accident= 2.018, Adjusted AATC = 1.397\n",
      "Recall@80%, Time to accident = 2.538\n",
      "Recall@80%, Precision = 0.4721\n",
      "map_location = <function <lambda> at 0x7f6c43c8c510>\n",
      "loading in  ./model/SASHA/FC_True_Future/chkpt-39.tar\n",
      "start_epoch = 40\n",
      "Using features from path: ./dataset/features/testing/\n",
      "Average Precision= 0.6321, Mean time to accident= 1.879, Adjusted AATC = 1.284\n",
      "Recall@80%, Time to accident = 2.752\n",
      "Recall@80%, Precision = 0.4364\n",
      "map_location = <function <lambda> at 0x7f6c641a91e0>\n",
      "loading in  ./model/SASHA/FC_True_Future/chkpt-44.tar\n",
      "start_epoch = 45\n",
      "Using features from path: ./dataset/features/testing/\n",
      "Average Precision= 0.6390, Mean time to accident= 2.156, Adjusted AATC = 1.535\n",
      "Recall@80%, Time to accident = 2.787\n",
      "Recall@80%, Precision = 0.432\n",
      "map_location = <function <lambda> at 0x7f6c414ff620>\n",
      "loading in  ./model/SASHA/FC_True_Future/chkpt-49.tar\n",
      "start_epoch = 50\n",
      "Using features from path: ./dataset/features/testing/\n",
      "Average Precision= 0.6178, Mean time to accident= 2.564, Adjusted AATC = 1.84\n",
      "Recall@80%, Time to accident = 3.448\n",
      "Recall@80%, Precision = 0.4058\n",
      "map_location = <function <lambda> at 0x7f6c43c8c510>\n",
      "loading in  ./model/SASHA/FC_True_Future/chkpt-54.tar\n",
      "start_epoch = 55\n",
      "Using features from path: ./dataset/features/testing/\n",
      "Average Precision= 0.5656, Mean time to accident= 2.585, Adjusted AATC = 1.926\n",
      "Recall@80%, Time to accident = 3.114\n",
      "Recall@80%, Precision = 0.4472\n",
      "map_location = <function <lambda> at 0x7f6c4134b2f0>\n",
      "loading in  ./model/SASHA/FC_True_Future/chkpt-59.tar\n",
      "start_epoch = 60\n",
      "Using features from path: ./dataset/features/testing/\n",
      "Average Precision= 0.6260, Mean time to accident= 2.711, Adjusted AATC = 1.966\n",
      "Recall@80%, Time to accident = 3.186\n",
      "Recall@80%, Precision = 0.4379\n",
      "map_location = <function <lambda> at 0x7f6c41381378>\n",
      "loading in  ./model/SASHA/FC_True_Future/chkpt-64.tar\n",
      "start_epoch = 65\n",
      "Using features from path: ./dataset/features/testing/\n",
      "Average Precision= 0.5756, Mean time to accident= 2.837, Adjusted AATC = 2.084\n",
      "Recall@80%, Time to accident = 3.452\n",
      "Recall@80%, Precision = 0.4137\n",
      "map_location = <function <lambda> at 0x7f6c412d11e0>\n",
      "loading in  ./model/SASHA/FC_True_Future/chkpt-69.tar\n",
      "start_epoch = 70\n",
      "Using features from path: ./dataset/features/testing/\n",
      "Average Precision= 0.5362, Mean time to accident= 2.948, Adjusted AATC = 2.148\n",
      "Recall@80%, Time to accident = 3.409\n",
      "Recall@80%, Precision = 0.4164\n",
      "map_location = <function <lambda> at 0x7f6c41316378>\n",
      "loading in  ./model/SASHA/FC_True_Future/chkpt-74.tar\n",
      "start_epoch = 75\n",
      "Using features from path: ./dataset/features/testing/\n",
      "Average Precision= 0.4975, Mean time to accident= 3.223, Adjusted AATC = 2.484\n",
      "Recall@80%, Time to accident = 3.464\n",
      "Recall@80%, Precision = 0.4205\n",
      "map_location = <function <lambda> at 0x7f6c43c73ea0>\n",
      "loading in  ./model/SASHA/FC_True_Future/chkpt-79.tar\n",
      "start_epoch = 80\n",
      "Using features from path: ./dataset/features/testing/\n",
      "Average Precision= 0.5494, Mean time to accident= 3.078, Adjusted AATC = 2.238\n",
      "Recall@80%, Time to accident = 3.906\n",
      "Recall@80%, Precision = 0.3872\n",
      "map_location = <function <lambda> at 0x7f6c4139d378>\n",
      "loading in  ./model/SASHA/FC_True_Future/chkpt-84.tar\n",
      "start_epoch = 85\n",
      "Using features from path: ./dataset/features/testing/\n",
      "Average Precision= 0.4807, Mean time to accident= 3.501, Adjusted AATC = 2.755\n",
      "Recall@80%, Time to accident = 3.914\n",
      "Recall@80%, Precision = 0.3908\n",
      "map_location = <function <lambda> at 0x7f6c41475400>\n",
      "loading in  ./model/SASHA/FC_True_Future/chkpt-89.tar\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-41ce90cbfe5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading in '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m   \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m   \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model1_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lstm_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model/SASHA/FC_True_Future/chkpt-89.tar'"
     ]
    }
   ],
   "source": [
    "approx_ATTC = 2\n",
    "models = [25,30,35,40,45,50,55,60,65,70,75,80,85,90]\n",
    "iter1 = 0\n",
    "window_length = 40\n",
    "for m in models:\n",
    "  name = ''\n",
    "  # print(save_path)\n",
    "  # print(os.listdir(save_path))\n",
    "  import warnings\n",
    "  warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "  #do change these\n",
    "  load_pretrained = True\n",
    "  saved_epoch = str(m-1)\n",
    "\n",
    "  #dont change these\n",
    "  start_epoch = 0 # this gets modified depending on loaded model\n",
    "  loss_history = []\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "      map_location=lambda storage, loc: storage.cuda()\n",
    "  else:\n",
    "      map_location='cpu'\n",
    "  print('map_location =', map_location)\n",
    "  # map_location=torch.device('cpu')\n",
    "\n",
    "  #model1.load_state_dict(torch.load(save_path + os.listdir(save_path)[-3], map_location = map_location))\n",
    "  #lstm.load_state_dict(torch.load(save_path + os.listdir(save_path)[-2], map_location = map_location))\n",
    "  #linear_pred.load_state_dict(torch.load(save_path + os.listdir(save_path)[-1], map_location = map_location  ))\n",
    "\n",
    "  PATH = os.path.join(save_path, 'chkpt-' + str(saved_epoch) + '.tar')\n",
    "  print('loading in ', PATH)\n",
    "\n",
    "  checkpoint = torch.load(PATH)\n",
    "  model1.load_state_dict(checkpoint['model1_state_dict'])\n",
    "  lstm.load_state_dict(checkpoint['lstm_state_dict'])\n",
    "  linear_pred.load_state_dict(checkpoint['linear_pred_state_dict'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "  if 'loss_history' in checkpoint.keys():\n",
    "    loss_history = checkpoint['loss_history']\n",
    "\n",
    "  start_epoch = int(saved_epoch) + 1\n",
    "\n",
    "  #start = model_to_load.find('_')\n",
    "  #end = model_to_load.find('.')\n",
    "  #saved_epoch = model_to_load[start+1:end]\n",
    "  #start_epoch = int(saved_epoch) + 1 \n",
    "\n",
    "\n",
    "  print('start_epoch =', start_epoch)\n",
    "\n",
    "\n",
    "  test_all(test_num,test_path, phys_path_tst, 'testing',model1, lstm, linear_pred, approx_ATTC, window_length, name)\n",
    "  # test_all(test_num,test_path, phys_path_tst, 'testing',model1, lstm, linear_pred, approx_ATTC, window_length, name, debug = True, log_file = log_file)\n",
    "  \n",
    "  iter1 += 1\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xeL_9RCXff6X"
   },
   "outputs": [],
   "source": [
    "os.listdir(save_path)[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1495,
     "status": "error",
     "timestamp": 1572831404138,
     "user": {
      "displayName": "ALEXANDER SAFONOV",
      "photoUrl": "",
      "userId": "06108573154481941124"
     },
     "user_tz": 480
    },
    "id": "WsOmxzbgbIih",
    "outputId": "95282754-83cf-4159-ad81-35ffffb87886"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/SASHA/FC_True_Future/\n",
      "['chkpt-4.tar', 'chkpt-9.tar', 'chkpt-14.tar', 'chkpt-19.tar', 'chkpt-24.tar', 'chkpt-29.tar', 'chkpt-34.tar', 'chkpt-39.tar', 'chkpt-44.tar', 'chkpt-49.tar', 'chkpt-54.tar', 'chkpt-59.tar', 'chkpt-64.tar', 'chkpt-69.tar', 'chkpt-74.tar', 'chkpt-79.tar', 'chkpt-84.tar', 'log.txt']\n",
      "5\n",
      "(15, 16, 17)\n",
      "chkpt-79.tar\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0e2853af0a79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mlinear_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m  \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 839\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EmbedLayer:\n\tMissing key(s) in state_dict: \"linear_glob.weight\", \"linear_glob.bias\", \"linear_local.weight\", \"linear_local.bias\", \"linear_phys.weight\", \"linear_phys.bias\". \n\tUnexpected key(s) in state_dict: \"epoch\", \"model1_state_dict\", \"lstm_state_dict\", \"linear_pred_state_dict\", \"optimizer_state_dict\", \"loss_history\", \"readme\". "
     ]
    }
   ],
   "source": [
    "print(save_path)\n",
    "print(os.listdir(save_path))\n",
    "len_dir = int(len(os.listdir(save_path))/3) -1 \n",
    "\n",
    "for ind in range(len_dir):\n",
    "  ind = len_dir-ind\n",
    "  \n",
    "#   ind = int((len_dir-1)*3)\n",
    "  tup =(ind*3, ind*3+1, ind*3+2)\n",
    "  print(len_dir)\n",
    "  \n",
    "\n",
    "  print(tup)\n",
    "  \n",
    "  print(os.listdir(save_path)[tup[0]])\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "      map_location=lambda storage, loc: storage.cuda()\n",
    "  else:\n",
    "      map_location='cpu'\n",
    "      \n",
    "  approx_ATTC = 2\n",
    "  \n",
    "  map_location=torch.device('cpu')\n",
    "  model1.load_state_dict(torch.load(save_path + os.listdir(save_path)[tup[0]], map_location = map_location))\n",
    "  lstm.load_state_dict(torch.load(save_path + os.listdir(save_path)[tup[1]], map_location = map_location))\n",
    "  linear_pred.load_state_dict(torch.load(save_path + os.listdir(save_path)[tup[2]], map_location = map_location  ))\n",
    "  \n",
    "  name = 'ADALea_LSTM_' + str(ind*5) ### FIx this, so it get saved consistently\n",
    "  \n",
    "  test_all(test_num,test_path,model1, lstm, linear_pred, approx_ATTC, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U3_fWCe5EBsL"
   },
   "outputs": [],
   "source": [
    "print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RcCuHWjZ50_9"
   },
   "outputs": [],
   "source": [
    "\n",
    "if os.path.isdir(save_path) == False:\n",
    "    os.mkdir(save_path)\n",
    "torch.save(model1.state_dict(),save_path + model_name1 + '.pth')\n",
    "torch.save(lstm.state_dict(),save_path + model_name2 + '.pth')\n",
    "torch.save(linear_pred.state_dict(),save_path + model_name3 + '.pth')\n",
    "print(' :::: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VlNPT-i4Fvo0"
   },
   "outputs": [],
   "source": [
    "print(save_path)\n",
    "print(os.listdir(save_path))\n",
    "len_dir = len(os.listdir(save_path))/3\n",
    "ind = (len_dir-1)*3\n",
    "tup = (ind, ind+1, ind+2)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    map_location=lambda storage, loc: storage.cuda()\n",
    "else:\n",
    "    map_location='cpu'\n",
    "\n",
    "map_location=torch.device('cpu')\n",
    "model1.load_state_dict(torch.load(save_path + os.listdir(save_path)[-3], map_location = map_location))\n",
    "lstm.load_state_dict(torch.load(save_path + os.listdir(save_path)[-2], map_location = map_location))\n",
    "linear_pred.load_state_dict(torch.load(save_path + os.listdir(save_path)[-1], map_location = map_location  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uSl6d_sfyEDr"
   },
   "outputs": [],
   "source": [
    "model1.eval()\n",
    "lstm.eval()\n",
    "linear_pred.eval()\n",
    "\n",
    "## Run model for a specific batch\n",
    "# for epoch in range(n_epochs):\n",
    "#      epoch = epoch + 39\n",
    "     # random chose batch.npz\n",
    "epoch_loss = np.zeros((train_num,1),dtype = float)\n",
    "n_batchs = np.arange(1,train_num+1)\n",
    "#      np.random.shuffle(n_batchs)\n",
    "tStart_epoch = time.time()\n",
    "# for batch in n_batchs:\n",
    "batch = n_batchs[1]\n",
    "\n",
    "file_name = '%03d' %batch\n",
    "batch_data = np.load(train_path+'batch_'+file_name+'.npz', allow_pickle = True)\n",
    "batch_xs = batch_data['data']\n",
    "batch_ys = batch_data['labels']\n",
    "IDs = batch_data['ID']\n",
    "#    optimizer.zero_grad()\n",
    "if torch.cuda.is_available():\n",
    "  batch_loss, all_alphas, soft_pred = run_batch(torch.from_numpy(batch_xs).float().cuda(), torch.from_numpy(batch_ys).float().cuda(), model1, lstm, linear_pred, True)\n",
    "else:\n",
    "  batch_loss, all_alphas, soft_pred = run_batch(torch.from_numpy(batch_xs).float(), torch.from_numpy(batch_ys).float(),model1, lstm, linear_pred, True)\n",
    "print(batch_loss.shape, all_alphas.shape, soft_pred.shape)\n",
    "\n",
    "\n",
    "epoch_loss[batch-1] = batch_loss.detach().cpu().numpy()/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sUgRp4RJyERa"
   },
   "outputs": [],
   "source": [
    "\n",
    "from google.colab.patches import cv2_imshow\n",
    "import cv2\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "# modify to load specific batches\n",
    "for num_batch in range(1,test_num):\n",
    "    file_name = '%03d' %num_batch\n",
    "    all_data = np.load(demo_path+'batch_'+file_name+'.npz')\n",
    "    data = all_data['data']\n",
    "    labels = all_data['labels']\n",
    "    det = all_data['det']\n",
    "    ID = all_data['ID']\n",
    " \n",
    "    # run result\n",
    "#     [all_loss,pred,weight] = sess.run([loss,soft_pred,all_alphas], feed_dict={x: data, y: labels, keep: [0.0], eps: eps})\n",
    "    if torch.cuda.is_available():\n",
    "      loss, weight, pred = run_batch(torch.from_numpy(batch_xs).float().cuda(), torch.from_numpy(batch_ys).float().cuda(), model1, lstm, linear_pred, True)\n",
    "    else:\n",
    "      loss, weight, pred = run_batch(torch.from_numpy(batch_xs).float(), torch.from_numpy(batch_ys).float(),model1, lstm, linear_pred, True)\n",
    "    \n",
    "\n",
    "\n",
    "    rand_batch = 0\n",
    "\n",
    "\n",
    "    feat_path = '../../Anticipating-Accidents/dataset/features/'+a1+'/'\n",
    "    features_ls = os.listdir(feat_path)\n",
    "    rand_int = np.random.randint(len(features_ls)) ## Change this to select a specific feature to plot\n",
    "    cur_feat = np.load(feat_path+features_ls[rand_int])\n",
    "    feat_path_full = feat_path+features_ls[rand_int] \n",
    "    selected_batch = feat_path_full[-7:-4]\n",
    "    \n",
    "    labels = cur_feat['labels']\n",
    "    ID = cur_feat['ID']\n",
    "    det = cur_feat['det']\n",
    "\n",
    "    video_paths = retrieve_frame_dir1(labels,ID, a1, None)\n",
    "\n",
    "    rand_batch = np.random.randint(10)\n",
    "    bboxes = det[rand_batch]\n",
    "\n",
    "    if labels[rand_batch,1] ==1:\n",
    "        a2 = str2[0]\n",
    "    else:\n",
    "        a2 = str2[1]\n",
    "\n",
    "    cur_file = video_paths[rand_batch][-7:-1]\n",
    "    rgb_path = './dataset/videos/frames/'+a1 +'/'+a2 +'/'+cur_file + '/'\n",
    "    video_path =  './dataset/videos/'+a1 +'/'+a2 +'/'+cur_file +'.mp4'\n",
    "    # batch follows feature naming pattern, not videos\n",
    "    backproj_feat_path = '../../Anticipating-Accidents/preprocessed_features/4_backprop_xy/'+a1 +'/batch_'+selected_batch+'.npz'\n",
    "\n",
    "    phys_feat = np.load(backproj_feat_path)['data']\n",
    "    \n",
    "    \n",
    "    file_list = sorted(os.listdir(video_path))\n",
    "    for i in range(len(ID)):\n",
    "        if labels[i][1] == 1 :\n",
    "            counter = 0 \n",
    "            fig = plt.figure()\n",
    "            line1, = plt.plot(pred[i,0:90],linewidth=3.0)\n",
    "            plt.ylim(0, 1)\n",
    "            line2, = plt.plot(counter,pred[i,counter],'ro')\n",
    "            plt.ylabel('Probability')\n",
    "            plt.xlabel('Frame')\n",
    "            ############\n",
    "            file_name = ID[i].decode('utf-8')\n",
    "            bboxes = det[i] \n",
    "            new_weight = weight[:,:,i]*255\n",
    "            print(video_path)\n",
    "            \n",
    "            cap = cv2.VideoCapture(video_path+file_name+'.mp4') \n",
    "            ret, frame = cap.read() \n",
    "            while(ret):\n",
    "                line2.set_data(counter,pred[i,counter])\n",
    "\n",
    "                fig.canvas.draw()\n",
    "                img = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8,sep='')\n",
    "                img  = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "                img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                \n",
    "                \n",
    "              ###\n",
    "                attention_frame = np.zeros((frame.shape[0],frame.shape[1]),dtype = np.uint8)\n",
    "                now_weight = new_weight[counter,:]\n",
    "                new_bboxes = bboxes[counter,:,:]\n",
    "                index = np.argsort(now_weight)\n",
    "                for num_box in index:\n",
    "                    if now_weight[num_box]/255.0>0.4:\n",
    "                        cv2.rectangle(frame,(new_bboxes[num_box,0],new_bboxes[num_box,1]),(new_bboxes[num_box,2],new_bboxes[num_box,3]),(0,255,0),3)\n",
    "                    else:\n",
    "                        cv2.rectangle(frame,(new_bboxes[num_box,0],new_bboxes[num_box,1]),(new_bboxes[num_box,2],new_bboxes[num_box,3]),(255,0,0),2)\n",
    "                    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                    cv2.putText(frame,str(round(now_weight[num_box]/255.0*10000)/10000),(new_bboxes[num_box,0],new_bboxes[num_box,1]), font, 0.5,(0,0,255),1,cv2.LINE_AA)\n",
    "                    attention_frame[int(new_bboxes[num_box,1]):int(new_bboxes[num_box,3]),int(new_bboxes[num_box,0]):int(new_bboxes[num_box,2])] = now_weight[num_box]\n",
    "\n",
    "                attention_frame = cv2.applyColorMap(attention_frame, cv2.COLORMAP_HOT)\n",
    "                dst = cv2.addWeighted(frame,0.6,attention_frame,0.4,0)\n",
    "                top_left_label = str(counter+1) + ': ' + file_name +', ' + 'Positive' ## change this for showing negative clips\n",
    "                cv2.putText(dst,top_left_label ,(10,30), font, 1,(255,255,255),3)\n",
    "                ###\n",
    "                a1,a2,a3 = img.shape\n",
    "                ratio = dst.shape[0]/a1\n",
    "                w1 = int(a2*0.75/ratio)\n",
    "                h1 = int(a1*0.75/ratio)\n",
    "                re_img = cv2.resize(img,(w1,h1), interpolation = cv2.INTER_AREA)\n",
    "                print(dst[0:h1,w1-1:-1].shape, re_img.shape)\n",
    "                overlay = cv2.addWeighted(dst[0:h1,-w1-1:-1],0.3,re_img,0.7,0)\n",
    "                dst[0:h1,-w1-1:-1] = overlay\n",
    "                ####\n",
    "\n",
    "                cv2.imshow('result',dst)\n",
    "                c = cv2.waitKey(50)\n",
    "                ret, frame = cap.read()\n",
    "                if c == ord('q') and c == 27 and ret:\n",
    "                    break;\n",
    "                counter += 1\n",
    "\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "35rwMIwNyEdJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LRHLaDr9Q2-y"
   },
   "outputs": [],
   "source": [
    "print (\"Training\")\n",
    "test_all(train_num,train_path,model1, lstm, linear_pred)\n",
    "print (\"Testing\")\n",
    "test_all(test_num,test_path,model1, lstm, linear_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cbijtTv8qEAQ"
   },
   "outputs": [],
   "source": [
    "# Print out Precision/Recall Curves\n",
    "# Print out Predictions with videos \n",
    "# Look at other methods of relating TTC to accident detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9bPiMNRFk-D9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n94R9e6wQ7K9"
   },
   "outputs": [],
   "source": [
    "# for epoch in range(n_epochs):\n",
    "#      # random chose batch.npz\n",
    "#      epoch_loss = np.zeros((train_num,1),dtype = float)\n",
    "#      n_batchs = np.arange(1,train_num+1)\n",
    "#      np.random.shuffle(n_batchs)\n",
    "#      tStart_epoch = time.time()\n",
    "#      for batch in n_batchs:\n",
    "#          file_name = '%03d' %batch\n",
    "#          batch_data = np.load(train_path+'batch_'+file_name+'.npz', allow_pickle = True)\n",
    "#          batch_xs = batch_data['data']\n",
    "#          batch_ys = batch_data['labels']\n",
    "        \n",
    "        \n",
    "#          _,batch_loss = sess.run([optimizer,loss], feed_dict={x: batch_xs, y: batch_ys, keep: [0.5]})\n",
    "#          epoch_loss[batch-1] = batch_loss/batch_size\n",
    "        \n",
    "        \n",
    "#      # print one epoch\n",
    "#      print( \"Epoch:\", epoch+1, \" done. Loss:\", np.mean(epoch_loss) )\n",
    "#      tStop_epoch = time.time()\n",
    "#      print( \"Epoch Time Cost:\", round(tStop_epoch - tStart_epoch,2), \"s\" )\n",
    "#      sys.stdout.flush()\n",
    "# #      if (epoch+1) % 15 == 0:\n",
    "# #         saver.save(sess,save_path+\"model\", global_step = epoch+1)\n",
    "# #         print (\"Training\")\n",
    "# #         test_all(sess,train_num,train_path,x,keep,y,loss,lstm_variables,soft_pred)\n",
    "# #         print (\"Testing\")\n",
    "# #         test_all(sess,test_num,test_path,x,keep,y,loss,lstm_variables,soft_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yeb6lJsKHRps"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILiTf5yeMeA9"
   },
   "source": [
    "Last cell worked on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wdeV4Xgrd_kz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gx4CiC5ndui-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "edzksQh8du0W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3qw23cX3_sMW"
   },
   "source": [
    "Run existing model on features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "62cI2-Yu_oJH"
   },
   "outputs": [],
   "source": [
    "## retrieve the correct model\n",
    "batches = np.sort(os.listdir(train_path))\n",
    "batch_num = 0\n",
    "cur_batch = np.load(train_path + batches[batch_num])\n",
    "data = cur_batch['data']\n",
    "labels = cur_batch['labels']\n",
    "\n",
    "## import physical parameters\n",
    "hard_drive_path = '.'\n",
    "phys_path_trn = hard_drive_path + '/preprocessed_features/4_backprop_xy/training/'\n",
    "phys_path_tst = hard_drive_path + '/preprocessed_features/4_backprop_xy/testing/'\n",
    "phys_batch = np.load(phys_path_trn +batches[batch_num])\n",
    "phys_data = phys_batch['data']\n",
    "\n",
    "# [all_loss,pred,weight] = sess.run([loss,soft_pred,all_alphas], feed_dict={x: data, y: labels, keep: [0.0]})\n",
    "\n",
    "## retrieve a positive example, insert the input\n",
    "## output normal risk and loss, \n",
    "## launch fgsm attack\n",
    "## generate new risks\n",
    "# data_1 = data[]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "WC_appearance_4_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
